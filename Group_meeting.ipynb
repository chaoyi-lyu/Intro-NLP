{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> $\\textbf{A keyboard, how quaint. Hello Computer!}$ <center>\n",
    "<font size=\"5\"> <div style=\"text-align: right\"> —An introduction to natural language processing </div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=\"4\"> $\\textbf{Chaoyi Lyu}$ </font> </div>\n",
    "<div style=\"text-align: right\"> <font size=\"4\"> $\\textbf{03 May 2021}$ </font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font size=\"6\">  **Outline** </font>\n",
    " \n",
    "<span style='background:yellow'>\n",
    "<font size=\"4\">\n",
    "* Data collection <br>\n",
    "* Processing the data <br>\n",
    "* Feature engineering <br>\n",
    "* Analysis with machine learning\n",
    "</font>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font size=\"7\"> Part 1 Extracting the Data </font> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Data: The introduction to natural language processing on Wikipedia stored in different formats </font> \n",
    "<img src=\"file/wiki2.png\", style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **1. Word files** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Natural language processing\n",
      "Natural language processing (NLP) is a subfield of , , and  concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of  data. The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "\n",
      "Challenges in natural language processing frequently involve , , and .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "An  providing on a web page, an example of an application where natural language processing is a major\n",
      "component.[ 1]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "History\n",
      "\n",
      "  involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n",
      "\n",
      "Symbolic NLP (1950s - early 1990s)\n",
      "\n",
      "The premise of symbolic NLP is well-summarized by 's experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.\n",
      "\n",
      "1950s: The in 1954 involved fully of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s when the first systems were developed.\n",
      "1960s: Some notably successful natural language processing systems developed in the 1960s were , a natural language system working in restricted \"\" with restricted vocabularies, and , a simulation of a , written by between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "1970s: During the 1970s, many programmers began to write \"conceptual \", which structured real- world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first many were written (e.g., ).\n",
      "1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of as a computational [3] (e.g., in the ). Other lines of research were continued, e.g., the development of chatterbots with and . An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.[5]\n",
      "\n",
      "Statistical NLP (1990s - 2010s)\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of  algorithms for language processing. This was due to both the steady increase in computational power (see ) and the gradual lessening of the dominance of  theories of linguistics (e.g. ), whose theoretical underpinnings discouraged the sort of that underlies the machine-learning approach to language processing.[6]\n",
      "\n",
      "ng multilingual that had been produced by the and the  as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become ted with the desired answers or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than , and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the ), which can often make up for the inferior results if the algorithm used has a low enough to be practical.\n",
      "\n",
      "Neural NLP (present)\n",
      "\n",
      "In the 2010s, and -style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others. This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[12]\n",
      "\n",
      "Methods: Rules, statistics, neural networks\n",
      "\n",
      "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for .\n",
      "More recent systems based on algorithms have many advantages over hand-produced rules:\n",
      "\n",
      "The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.\n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable.\n",
      "However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.\n",
      "\n",
      "Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used\n",
      "\n",
      "when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the system,\n",
      "for preprocessing in NLP pipelines, e.g., , or\n",
      "for postprocessing and transforming the output of NLP pipelines, e.g., for from syntactic parses.\n",
      "\n",
      "Statistical methods\n",
      "Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using  to automatically learn such rules through the analysis of large (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\n",
      "\n",
      "Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on , which make soft, decisions based on attaching weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "Some of the earliest-used machine learning algorithms, such as , produced systems of hard if-then rules similar to existing hand-written rules. However,  introduced the use of  to natural language processing, and increasingly, research has focused on , which make soft, decisions based on attaching weights to the features making up the input data. The upon which many systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\n",
      "\n",
      "Neural networks\n",
      "\n",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[17] the field has thus largely abandoned statistical methods and shifted to for machine learning. Popular techniques include the use of to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term  (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in (SMT). Latest works\n",
      "tend to use non-technical structure of a given task to build proper neural network.[18]\n",
      "\n",
      "Common NLP tasks\n",
      "\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
      "\n",
      "Text and speech processing\n",
      "\n",
      "(OCR)\n",
      "Given an image representing printed text, determine the corresponding text.\n",
      "\n",
      "Given a sound clip of a person or people speaking, determine the textual representation of the speech.  thus is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed , so the conversion of the to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.\n",
      "\n",
      "Given a sound clip of a person or people speaking, separate it into words. A subtask of  and typically grouped with it.\n",
      "\n",
      "Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.[19]\n",
      "()\n",
      "Separate a chunk of continuous text into separate words. For a language like , this is fairly trivial, since words are usually separated by spaces. However, some written languages like ,  and do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the and of words in the language.\n",
      "Sometimes this process is also used in cases like (BOW) creation in data mining.\n",
      "\n",
      "Morphological analysis\n",
      "\n",
      "\n",
      "The task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.[20]\n",
      "\n",
      "Separate words into individual and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the (i.e., the structure of words) of the language being considered. has fairly simple morphology, especially , and thus it is\n",
      "\n",
      "often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as or ,[21] a highly Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\n",
      "\n",
      "Given a sentence, determine the (POS) for each word. Many words, especially common ones, can serve as multiple . For example, \"book\" can be a (\"the book on the table\") or (\"to book a flight\"); \"set\" can be a , or ; and \"out\" can be any of at least five different parts of speech.\n",
      "\n",
      "The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.\n",
      "\n",
      "Syntactic analysis\n",
      "[22]\n",
      "Generate a that describes a language's syntax.\n",
      "(also known as \"\")\n",
      "Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by or other , but these same characters can serve other purposes (e.g., marking ).\n",
      "\n",
      "Determine the (grammatical analysis) of a given sentence. The for  is and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing.\n",
      "Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a (PCFG) (see also ).\n",
      "\n",
      "Lexical semantics (of individual words in context)\n",
      "\n",
      "\n",
      "What is the computational meaning of individual words in context?\n",
      "\n",
      "How can we learn semantic representations from data?\n",
      "(NER)\n",
      "Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of , and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. or ) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, capitalizes all , regardless of whether they are names, and and do not capitalize names that serve as .\n",
      "(see also )\n",
      "Extract subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing.\n",
      "\n",
      "The goal of terminology extraction is to automatically extract relevant terms from a given corpus.\n",
      "\n",
      "Many words have more than one ; we have to select the meaning which makes the most sense in context. For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as .\n",
      "\n",
      "Relational semantics (semantics of individual sentences)\n",
      "\n",
      "\n",
      "Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\n",
      "\n",
      "Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in ) or in accordance with a logical formalism (e.g., in ). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).\n",
      "(see also implicit semantic role labelling below)\n",
      "Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal ), then identify and classify the frame elements ().\n",
      "\n",
      "Discourse (semantics beyond individual sentences)\n",
      "\n",
      "\n",
      "Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). is a specific example of this task, and is specifically concerned with matching up with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving . For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).\n",
      "\n",
      "This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the  structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast). Another possible task is recognizing and classifying the in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).\n",
      "Implicit semantic role labelling\n",
      "Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal ) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local \n",
      "\n",
      "Given two text fragments, determine if one being true entails the other, entails the other's negation, or allows the other to be either true or false.[23]\n",
      "and recognition\n",
      "Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.\n",
      "\n",
      "The goal of argument mining is the automatic extraction and identification of argumentative structures from text with the aid of computer programs. [24] Such argumentative structures include the premise, conclusions, the and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse. [25][26]\n",
      "\n",
      "Higher-level NLP applications\n",
      "\n",
      "(text summarization)\n",
      "Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.\n",
      "Book generation\n",
      "Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in\n",
      "\n",
      "1984 (Racter, The policeman's beard is half-constructed).[27] The first published work by a neural network was published in 2018, , marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) . The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[28] Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.\n",
      "\n",
      "Computer systems intended to converse with a human.\n",
      "\n",
      "A Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.[29]\n",
      "Grammatical error correction\n",
      "Grammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011.[30][31][32] As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as , this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.[33]\n",
      "\n",
      "Automatically translate text from one human language to another. This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.\n",
      "(NLG):\n",
      "Convert information from computer databases or semantic intents into readable human language.\n",
      "(NLU)\n",
      "Convert chunks of text into more formal representations such as structures that are easier for programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit  expected for the construction of a basis of semantics formalization.[34]\n",
      "\n",
      "Given a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\"). Recent works have looked at even more complex questions.[35]\n",
      "\n",
      "General tendencies and (possible) future directions\n",
      "\n",
      "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[36]\n",
      "\n",
      "Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999-2001: shallow parsing, 2002- 03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing).\n",
      "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\n",
      "Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\n",
      "\n",
      "Cognition and NLP\n",
      "\n",
      "Most more higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
      "\n",
      "refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience,  linguistics.[39] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n",
      "\n",
      "As an example, offers a methodology to build natural language processing (NLP) algorithms through the perspective of , along with the findings of ,[40] with two defining aspects:\n",
      "Apply the theory of , explained by Lakoff as “the understanding of one idea, in terms of another” which provides an idea of the intent of the author.[41] For example, consider the English word “big”. When used in a comparison (“That is a big tree”), the author's intent is to imply that the tree is ”physically large” relative to other trees or the authors experience. When used metaphorically (”Tomorrow is a big day”), the author’s intent to imply ”importance”. The intent behind other usages, like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\n",
      "Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information  \n",
      "\n",
      "\n",
      "Where,\n",
      "RMM, is the Relative Measure of Meaning\n",
      "token, is any block of text, sentence, phrase or word\n",
      "N, is the number of tokens being analyzed\n",
      "PMM, is the Probable Measure of Meaning based on a corpora d, is the location of the token along the sequence of N-1 tokens PF, is the Probability Function specific to a language\n",
      "\n",
      "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[42]  recently, ideas of cognitive NLP have been revived as an approach to achieve , e.g., under the notion of \"cognitive AI\".[46] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[47]\n",
      "\n",
      "See also\n",
      "\n",
      "\n",
      "      \n",
      " \n",
      "\n",
      "   \n",
      " \n",
      "   \n",
      "\n",
      " \n",
      "   \n",
      "\n",
      "References\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Kongthon, Alisa; Sangkeettrakarn, Chatchawal; Kongyoung, Sarawoot; Haruechaiyasak, Choochart (October 27–30, 2009). Implementing an online help desk system based on conversational agent. MEDES '09: The International Conference on Management of Emergent Digital EcoSystems. France: ACM. :.\n",
      "\n",
      "PDF), Department of General Linguistics, \n",
      "\n",
      "Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and \n",
      "30688575 (https://api.semanticscholar.org/CorpusID:30688575).\n",
      "Chomskyan linguistics encourages the investigation of \"\" that stress the limits of its theoretical d data, as is the case in . The creation and use of such of real-world data is a fundamental part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings of Chomskyan linguistics such as the so-called \"\" argument entail that general learning algorithms, as are typically used in machine learning, cannot be successful in language processing. As a result, the Chomskyan paradigm discouraged the application of such models to language processing.\n",
      "Goldberg, Yoav (2016). \"A Primer on Neural Network Models for Natural Language Processing\". Journal of Artificial Intelligence Research. 57: 345–420. :. :. 8273530 (https://api.semanticscholar.or g/CorpusID:8273530).\n",
      "\n",
      "Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). Exploring the Limits of Language Modeling. :. :.\n",
      "\n",
      " :.\n",
      " \n",
      "\n",
      "Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum. 0-470-99033-3.\n",
      "\n",
      " Computational Linguistics.\n",
      "Language Log, February 5, 2011.\n",
      " Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability. Until 2015, deep learning had evolved into the major framework of NLP.\n",
      "\n",
      "Yi, Chucai; Tian, Yingli (2012), \"Assistive Text Reading from Complex Background for Blind Persons\", Camera-Based Document Analysis and Recognition, Springer Berlin Heidelberg, pp. 15–28, , :,\n",
      "9783642293634\n",
      "\n",
      " Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012: 95–108.\n",
      "\n",
      "PASCAL Recognizing Textual Entailment Challenge (RTE-7) \n",
      " \n",
      ".\n",
      "Retrieved 2021-03-09.\n",
      ". Retrieved 2021-03-09.\n",
      ". Retrieved 2020-08-17.\n",
      "28. \n",
      "\n",
      "\n",
      " University. Retrieved 2021-01-11.\n",
      ".\n",
      "Retrieved 2021-01-11.\n",
      ".\n",
      "Retrieved 2021-01-11.\n",
      ". Retrieved 2021-01-11.\n",
      "Duan, Yucong; Cruz, Christophe (2011). \"Formalizing Semantic of Natural Language through Conceptualization from Existence\" (https://web.archive.org/web/20111009135952/ ct/100-E00187.htm). International Journal of Innovation, Management and Technology. 2 (1): 37–42. Archived from on 2011-10-09.\n",
      " \n",
      ". Retrieved 2021-01-11.\n",
      "\n",
      ". Lexico. and . Retrieved 6 May 2020.\n",
      ". American Federation of Teachers. 8 August 2014. \"Cognitive science is an interdisciplinary field of researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology that seek to understand the mind.\"\n",
      "Robinson, Peter (2008). Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge. pp. 3–8. 978-0-805-85352-0.\n",
      "Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp. 569–583. 978-0-465-05674-3.\n",
      "Strauss, Claudia (1999). A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp. 156– 164. 978-0-521-59541-4.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ". Retrieved 2021-01-11.\n",
      " 207–218. :. 2317858 (https://api. semanticscholar.org/CorpusID:2317858).\n",
      "\n",
      "Further reading\n",
      " 9982. :.\n",
      "\n",
      "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. 978-0-596-51649-9.\n",
      "Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. 978-0-13-187321-6.\n",
      "Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley. 978-1848218482.\n",
      "Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley. 978-1848219212.\n",
      "Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information \n",
      "Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. 978-0-262-13360-9.\n",
      "David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer- Verlag. 978-0-387-19557-5.\n",
      "\n",
      "Retrieved from \"\"\n",
      "\n",
      "\n",
      "This page was last edited on 26 April 2021, at 23:57 (UTC).\n",
      "\n",
      "Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the and . Wikipedia® is a registered trademark of the , a non-profit organization.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = open(\"file/Natural_language_processing.docx\",\"rb\")\n",
    "document = Document(doc)\n",
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "       docu += para.text +'\\n'\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **2. PDF files** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pdfminer rather than PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and\n",
      "artificial  intelligence  concerned  with  the  interactions  between  computers  and  human\n",
      "language, in particular how to program computers to process and analyze large amounts\n",
      "of  natural  language  data.  The  result  is  a  computer  capable  of  \"understanding\"  the\n",
      "contents of documents, including the contextual nuances of the language within them.\n",
      "The  technology  can  then  accurately  extract  information  and  insights  contained  in  the\n",
      "documents as well as categorize and organize the documents themselves.\n",
      "\n",
      "Challenges  in  natural  language  processing  frequently  involve  speech  recognition,\n",
      "natural language understanding, and natural-language generation.\n",
      "\n",
      "Contents\n",
      "History\n",
      "\n",
      "Symbolic NLP (1950s - early 1990s)\n",
      "Statistical NLP (1990s - 2010s)\n",
      "Neural NLP (present)\n",
      "\n",
      "Methods: Rules, statistics, neural networks\n",
      "\n",
      "Statistical methods\n",
      "Neural networks\n",
      "Common NLP tasks\n",
      "\n",
      "Text and speech processing\n",
      "Morphological analysis\n",
      "Syntactic analysis\n",
      "Lexical semantics (of individual words in context)\n",
      "Relational semantics (semantics of individual sentences)\n",
      "Discourse (semantics beyond individual sentences)\n",
      "Higher-level NLP applications\n",
      "\n",
      "General tendencies and (possible) future directions\n",
      "\n",
      "Cognition and NLP\n",
      "\n",
      "See also\n",
      "References\n",
      "Further reading\n",
      "\n",
      "History\n",
      "\n",
      "An automated online assistant\n",
      "providing customer service on a\n",
      "web page, an example of an\n",
      "application where natural\n",
      "language processing is a major\n",
      "component.[1]\n",
      "\n",
      "Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing\n",
      "Machinery and Intelligence\"  which  proposed  what  is  now  called  the  Turing test  as  a  criterion  of  intelligence,  a  task  that\n",
      "involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate\n",
      "from artificial intelligence.\n",
      "\n",
      "Symbolic NLP (1950s - early 1990s)\n",
      "\n",
      "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules\n",
      "(e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding\n",
      "(or other NLP tasks) by applying those rules to the data it is confronted with.\n",
      "\n",
      "\f",
      "1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian\n",
      "sentences into English. The authors claimed that within three or five years, machine translation would be a\n",
      "solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which\n",
      "found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was\n",
      "dramatically reduced. Little further research in machine translation was conducted until the late 1980s when\n",
      "the first statistical machine translation systems were developed.\n",
      "1960s: Some notably successful natural language processing systems developed in the 1960s were\n",
      "SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and\n",
      "ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and\n",
      "1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly\n",
      "human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a\n",
      "generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-\n",
      "world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM\n",
      "(Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics\n",
      "(Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, the first many chatterbots were written\n",
      "(e.g., PARRY).\n",
      "1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. Focus areas of the time\n",
      "included research on rule-based parsing (e.g., the development of HPSG as a computational\n",
      "operationalization of generative grammar), morphology (e.g., two-level morphology[3]), semantics (e.g., Lesk\n",
      "algorithm), reference (e.g., within Centering Theory[4]) and other areas of natural language understanding\n",
      "(e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of\n",
      "chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical\n",
      "turn in the 1990s) was the rising importance of quantitative evaluation in this period.[5]\n",
      "\n",
      "Statistical NLP (1990s - 2010s)\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the\n",
      "late  1980s,  however,  there  was  a  revolution  in  natural  language  processing  with  the  introduction  of  machine  learning\n",
      "algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and\n",
      "the  gradual  lessening  of  the  dominance  of  Chomskyan  theories  of  linguistics  (e.g.  transformational  grammar),  whose\n",
      "theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language\n",
      "processing.[6]\n",
      "\n",
      "1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine\n",
      "translation, due especially to work at IBM Research. These systems were able to take advantage of existing\n",
      "multilingual textual corpora that had been produced by the Parliament of Canada and the European Union\n",
      "as a result of laws calling for the translation of all governmental proceedings into all official languages of the\n",
      "corresponding systems of government. However, most other systems depended on corpora specifically\n",
      "developed for the tasks implemented by these systems, which was (and often continues to be) a major\n",
      "limitation in the success of these systems. As a result, a great deal of research has gone into methods of\n",
      "more effectively learning from limited amounts of data.\n",
      "2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become\n",
      "available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-\n",
      "supervised learning algorithms. Such algorithms can learn from data that has not been hand-annotated with\n",
      "the desired answers or using a combination of annotated and non-annotated data. Generally, this task is\n",
      "much more difficult than supervised learning, and typically produces less accurate results for a given amount\n",
      "of input data. However, there is an enormous amount of non-annotated data available (including, among\n",
      "other things, the entire content of the World Wide Web), which can often make up for the inferior results if the\n",
      "algorithm used has a low enough time complexity to be practical.\n",
      "\n",
      "Neural NLP (present)\n",
      "\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural\n",
      "language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in\n",
      "many  natural  language  tasks,  for  example  in  language  modeling,[9] parsing,[10][11]  and  many  others.  This  is  increasingly\n",
      "important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that\n",
      "would otherwise be inaccessible for study when seeking to improve care.[12]\n",
      "\n",
      "\f",
      "Methods: Rules, statistics, neural networks\n",
      "\n",
      "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of\n",
      "rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.\n",
      "\n",
      "More recent systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
      "\n",
      "The learning procedures used during machine learning automatically focus on the most common cases,\n",
      "whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are\n",
      "robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to\n",
      "erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input\n",
      "gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft\n",
      "decisions, is extremely difficult, error-prone and time-consuming.\n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more\n",
      "input data. However, systems based on handwritten rules can only be made more accurate by increasing the\n",
      "complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of\n",
      "systems based on handwritten rules, beyond which the systems become more and more unmanageable.\n",
      "However, creating more data to input to machine-learning systems simply requires a corresponding increase\n",
      "in the number of man-hours worked, generally without significant increases in the complexity of the\n",
      "annotation process.\n",
      "\n",
      "Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used\n",
      "\n",
      "when the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the\n",
      "machine translation of low-resource languages such as provided by the Apertium system,\n",
      "for preprocessing in NLP pipelines, e.g., tokenization, or\n",
      "for postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from\n",
      "syntactic parses.\n",
      "\n",
      "Statistical methods\n",
      "\n",
      "Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research\n",
      "has  relied  heavily  on  machine  learning.  The  machine-learning  paradigm  calls  instead  for  using  statistical  inference  to\n",
      "automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly\n",
      "with human or computer annotations) of typical real-world examples.\n",
      "\n",
      "Many  different  classes  of  machine-learning  algorithms  have  been  applied  to  natural-language-processing  tasks.  These\n",
      "algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has\n",
      "focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input\n",
      "feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather\n",
      "than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar\n",
      "to  existing  hand-written  rules.  However, part-of-speech tagging  introduced  the  use  of  hidden  Markov  models  to  natural\n",
      "language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions\n",
      "based on attaching real-valued weights to the features making up the input data. The cache language models upon which\n",
      "many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust\n",
      "when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more\n",
      "reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they\n",
      "continue to be relevant for contexts in which statistical interpretability and transparency is required.\n",
      "\n",
      "Neural networks\n",
      "\n",
      "\f",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[17] the field has thus\n",
      "largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use\n",
      "of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task\n",
      "(e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and\n",
      "dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that\n",
      "deep  neural  network-based  approaches  may  be  viewed  as  a  new  paradigm  distinct  from  statistical  natural  language\n",
      "processing.  For  instance,  the  term  neural  machine  translation  (NMT)  emphasizes  the  fact  that  deep  learning-based\n",
      "approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate\n",
      "steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works\n",
      "tend to use non-technical structure of a given task to build proper neural network.[18]\n",
      "\n",
      "Common NLP tasks\n",
      "\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks\n",
      "have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A\n",
      "coarse division is given below.\n",
      "\n",
      "Text and speech processing\n",
      "\n",
      "Optical character recognition (OCR)\n",
      "\n",
      "Given an image representing printed text, determine the corresponding text.\n",
      "\n",
      "Speech recognition\n",
      "\n",
      "Given a sound clip of a person or people speaking, determine the textual representation of the speech.\n",
      "This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-\n",
      "complete\" (see above). In natural speech there are hardly any pauses between successive words, and\n",
      "thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken\n",
      "languages, the sounds representing successive letters blend into each other in a process termed\n",
      "coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process.\n",
      "Also, given that words in the same language are spoken by people with different accents, the speech\n",
      "recognition software must be able to recognize the wide variety of input as being identical to each other in\n",
      "terms of its textual equivalent.\n",
      "\n",
      "Given a sound clip of a person or people speaking, separate it into words. A subtask of speech recognition\n",
      "and typically grouped with it.\n",
      "\n",
      "Speech segmentation\n",
      "\n",
      "Text-to-speech\n",
      "\n",
      "Given a text, transform those units and produce a spoken representation. Text-to-speech can be used to\n",
      "aid the visually impaired.[19]\n",
      "\n",
      "Word segmentation (Tokenization)\n",
      "\n",
      "Separate a chunk of continuous text into separate words. For a language like English, this is fairly trivial,\n",
      "since words are usually separated by spaces. However, some written languages like Chinese, Japanese\n",
      "and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a\n",
      "significant task requiring knowledge of the vocabulary and morphology of words in the language.\n",
      "Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.\n",
      "\n",
      "Morphological analysis\n",
      "\n",
      "Lemmatization\n",
      "\n",
      "The task of removing inflectional endings only and to return the base dictionary form of a word which is\n",
      "also known as a lemma. Lemmatization is another technique for reducing words to their normalized form.\n",
      "But in this case, the transformation actually uses a dictionary to map words to their actual form.[20]\n",
      "\n",
      "Morphological segmentation\n",
      "\n",
      "Separate words into individual morphemes and identify the class of the morphemes. The difficulty of this\n",
      "task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language\n",
      "being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is\n",
      "\n",
      "\f",
      "often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open,\n",
      "opens, opened, opening\") as separate words. In languages such as Turkish or Meitei,[21] a highly\n",
      "agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has\n",
      "thousands of possible word forms.\n",
      "\n",
      "Part-of-speech tagging\n",
      "\n",
      "Given a sentence, determine the part of speech (POS) for each word. Many words, especially common\n",
      "ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\")\n",
      "or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five\n",
      "different parts of speech.\n",
      "\n",
      "Stemming\n",
      "\n",
      "The process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the\n",
      "root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but\n",
      "does so on grounds of rules, not a dictionary.\n",
      "\n",
      "Syntactic analysis\n",
      "\n",
      "Grammar induction[22]\n",
      "\n",
      "Generate a formal grammar that describes a language's syntax.\n",
      "\n",
      "Sentence breaking (also known as \"sentence boundary disambiguation\")\n",
      "\n",
      "Parsing\n",
      "\n",
      "Given a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or\n",
      "other punctuation marks, but these same characters can serve other purposes (e.g., marking\n",
      "abbreviations).\n",
      "\n",
      "Determine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages\n",
      "is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical\n",
      "sentence there may be thousands of potential parses (most of which will seem completely nonsensical to\n",
      "a human). There are two primary types of parsing: dependency parsing and constituency parsing.\n",
      "Dependency parsing focuses on the relationships between words in a sentence (marking things like\n",
      "primary objects and predicates), whereas constituency parsing focuses on building out the parse tree\n",
      "using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).\n",
      "\n",
      "Lexical semantics (of individual words in context)\n",
      "\n",
      "Lexical semantics\n",
      "\n",
      "Distributional semantics\n",
      "\n",
      "What is the computational meaning of individual words in context?\n",
      "\n",
      "How can we learn semantic representations from data?\n",
      "\n",
      "Named entity recognition (NER)\n",
      "\n",
      "Given a stream of text, determine which items in the text map to proper names, such as people or places,\n",
      "and what the type of each such name is (e.g. person, location, organization). Although capitalization can\n",
      "aid in recognizing named entities in languages such as English, this information cannot aid in determining\n",
      "the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a\n",
      "sentence is also capitalized, and named entities often span several words, only some of which are\n",
      "capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not\n",
      "have any capitalization at all, and even languages with capitalization may not consistently use it to\n",
      "distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and\n",
      "French and Spanish do not capitalize names that serve as adjectives.\n",
      "\n",
      "Sentiment analysis (see also Multimodal sentiment analysis)\n",
      "\n",
      "Extract subjective information usually from a set of documents, often using online reviews to determine\n",
      "\"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in social\n",
      "media, for marketing.\n",
      "\n",
      "Terminology extraction\n",
      "\n",
      "The goal of terminology extraction is to automatically extract relevant terms from a given corpus.\n",
      "\n",
      "Word sense disambiguation\n",
      "\n",
      "Many words have more than one meaning; we have to select the meaning which makes the most sense in\n",
      "context. For this problem, we are typically given a list of words and associated word senses, e.g. from a\n",
      "dictionary or an online resource such as WordNet.\n",
      "\n",
      "\f",
      "Relational semantics (semantics of individual sentences)\n",
      "\n",
      "Relationship extraction\n",
      "\n",
      "Semantic parsing\n",
      "\n",
      "Given a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\n",
      "\n",
      "Given a piece of text (typically a sentence), produce a formal representation of its semantics, either as a\n",
      "graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This\n",
      "challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic\n",
      "role labelling, word sense disambiguation) and can be extended to include full-fledged discourse analysis\n",
      "(e.g., discourse analysis, coreference; see Natural language understanding below).\n",
      "\n",
      "Semantic role labelling (see also implicit semantic role labelling below)\n",
      "\n",
      "Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify\n",
      "and classify the frame elements (semantic roles).\n",
      "\n",
      "Discourse (semantics beyond individual sentences)\n",
      "\n",
      "Coreference resolution\n",
      "\n",
      "Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects\n",
      "(\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with\n",
      "matching up pronouns with the nouns or names to which they refer. The more general task of coreference\n",
      "resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For\n",
      "example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a\n",
      "referring expression and the bridging relationship to be identified is the fact that the door being referred to\n",
      "is the front door of John's house (rather than of some other structure that might also be referred to).\n",
      "\n",
      "This rubric includes several related tasks. One task is discourse parsing, i.e., identifying the discourse\n",
      "structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g.\n",
      "elaboration, explanation, contrast). Another possible task is recognizing and classifying the speech acts in\n",
      "a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).\n",
      "\n",
      "Discourse analysis\n",
      "\n",
      "Implicit semantic role labelling\n",
      "\n",
      "Given a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their\n",
      "explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic\n",
      "roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly\n",
      "realized elsewhere in the text and those that are not specified, and resolve the former against the local\n",
      "text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-\n",
      "drop languages.\n",
      "\n",
      "Recognizing textual entailment\n",
      "\n",
      "Given two text fragments, determine if one being true entails the other, entails the other's negation, or\n",
      "allows the other to be either true or false.[23]\n",
      "\n",
      "Topic segmentation and recognition\n",
      "\n",
      "Given a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic\n",
      "of the segment.\n",
      "\n",
      "Argument mining\n",
      "\n",
      "The goal of argument mining is the automatic extraction and identification of argumentative structures from\n",
      "natural language text with the aid of computer programs. [24] Such argumentative structures include the\n",
      "premise, conclusions, the argument scheme and the relationship between the main and subsidiary\n",
      "argument, or the main and counter-argument within discourse. [25][26]\n",
      "\n",
      "Higher-level NLP applications\n",
      "\n",
      "Automatic summarization (text summarization)\n",
      "\n",
      "Produce a readable summary of a chunk of text. Often used to provide summaries of the text of a known\n",
      "type, such as research papers, articles in the financial section of a newspaper.\n",
      "\n",
      "Book generation\n",
      "\n",
      "Not an NLP task proper but an extension of natural language generation and other NLP tasks is the\n",
      "creation of full-fledged books. The first machine-generated book was created by a rule-based system in\n",
      "\n",
      "\f",
      "1984 (Racter, The policeman's beard is half-constructed).[27] The first published work by a neural network\n",
      "was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems\n",
      "are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated\n",
      "science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham).[28] Unlike\n",
      "Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.\n",
      "\n",
      "Dialogue management\n",
      "\n",
      "Document AI\n",
      "\n",
      "Computer systems intended to converse with a human.\n",
      "\n",
      "A Document AI platform sits on top of the NLP technology enabling users with no prior experience of\n",
      "artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they\n",
      "need from different document types. NLP-powered Document AI enables non-technical teams to quickly\n",
      "access information hidden in documents, for example, lawyers, business analysts and accountants.[29]\n",
      "\n",
      "Grammatical error correction\n",
      "\n",
      "Grammatical error detection and correction involves a great band-width of problems on all levels of\n",
      "linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error\n",
      "correction is impactful since it affects hundreds of millions of people that use or acquire English as a\n",
      "second language. It has thus been subject to a number of shared tasks since 2011.[30][31][32] As far as\n",
      "orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the\n",
      "development of powerful neural language models such as GPT-2, this can now (2019) be considered a\n",
      "largely solved problem and is being marketed in various commercial applications.[33]\n",
      "\n",
      "Machine translation\n",
      "\n",
      "Automatically translate text from one human language to another. This is one of the most difficult problems,\n",
      "and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different\n",
      "types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve\n",
      "properly.\n",
      "\n",
      "Natural language generation (NLG):\n",
      "\n",
      "Natural language understanding (NLU)\n",
      "\n",
      "Convert information from computer databases or semantic intents into readable human language.\n",
      "\n",
      "Convert chunks of text into more formal representations such as first-order logic structures that are easier\n",
      "for computer programs to manipulate. Natural language understanding involves the identification of the\n",
      "intended semantic from the multiple possible semantics which can be derived from a natural language\n",
      "expression which usually takes the form of organized notations of natural language concepts. Introduction\n",
      "and creation of language metamodel and ontology are efficient however empirical solutions. An explicit\n",
      "formalization of natural language semantics without confusions with implicit assumptions such as closed-\n",
      "world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is\n",
      "expected for the construction of a basis of semantics formalization.[34]\n",
      "\n",
      "Question answering\n",
      "\n",
      "Given a human-language question, determine its answer. Typical questions have a specific right answer\n",
      "(such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered\n",
      "(such as \"What is the meaning of life?\"). Recent works have looked at even more complex questions.[35]\n",
      "\n",
      "General tendencies and (possible) future directions\n",
      "\n",
      "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends\n",
      "among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[36]\n",
      "\n",
      "Interest on increasingly abstract, \"cognitive\" aspects of natural language (1999-2001: shallow parsing, 2002-\n",
      "03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role\n",
      "labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing).\n",
      "Increasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch\n",
      "since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish\n",
      "since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009;\n",
      "Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\n",
      "Elimination of symbolic representations (rule-based over supervised towards weakly supervised methods,\n",
      "representation learning and end-to-end systems)\n",
      "\n",
      "Cognition and NLP\n",
      "\n",
      "\f",
      "Most more higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of\n",
      "natural  language.  More  broadly  speaking,  the  technical  operationalization  of  increasingly  advanced  aspects  of  cognitive\n",
      "behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
      "\n",
      "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience,\n",
      "and the senses.\"[37] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[38] Cognitive\n",
      "linguistics  is  an  interdisciplinary  branch  of  linguistics,  combining  knowledge  and  research  from  both  psychology  and\n",
      "linguistics.[39] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with\n",
      "cognitive studies.\n",
      "\n",
      "As an example, George Lakoff  offers  a  methodology  to  build  natural  language  processing  (NLP)  algorithms  through  the\n",
      "perspective of cognitive science, along with the findings of cognitive linguistics,[40] with two defining aspects:\n",
      "\n",
      "1. Apply the theory of conceptual metaphor, explained by Lakoff as “the understanding of one idea, in terms of\n",
      "another” which provides an idea of the intent of the author.[41] For example, consider the English word “big”.\n",
      "When used in a comparison (“That is a big tree”), the author's intent is to imply that the tree is ”physically\n",
      "large” relative to other trees or the authors experience. When used metaphorically (”Tomorrow is a big day”),\n",
      "the author’s intent to imply ”importance”. The intent behind other usages, like in ”She is a big person” will\n",
      "remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional\n",
      "information.\n",
      "\n",
      "2. Assign relative measures of meaning to a word, phrase, sentence or piece of text based on the information\n",
      "\n",
      "presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free\n",
      "grammar (PCFG). The mathematical equation for such algorithms is presented in US patent 9269353 (http\n",
      "s://worldwide.espacenet.com/textdoc?DB=EPODOC&IDX=US9269353):\n",
      "\n",
      "Where,\n",
      "\n",
      "RMM, is the Relative Measure of Meaning\n",
      "token, is any block of text, sentence, phrase or word\n",
      "N, is the number of tokens being analyzed\n",
      "PMM, is the Probable Measure of Meaning based on a corpora\n",
      "d, is the location of the token along the sequence of N-1 tokens\n",
      "PF, is the Probability Function specific to a language\n",
      "\n",
      "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since\n",
      "the  statistical  turn  during  the  1990s.  Nevertheless,  approaches  to  develop  cognitive  models  towards  technically\n",
      "operationalizable  frameworks  have  been  pursued  in  the  context  of  various  frameworks,  e.g.,  of  cognitive  grammar,[42]\n",
      "functional grammar,[43] construction grammar,[44] computational psycholinguistics and cognitive neuroscience (e.g., ACT-\n",
      "R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[45] of the ACL). More\n",
      "recently,  ideas  of  cognitive  NLP  have  been  revived  as  an  approach  to  achieve  explainability,  e.g.,  under  the  notion  of\n",
      "\"cognitive AI\".[46] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made\n",
      "explicit).[47]\n",
      "\n",
      "See also\n",
      "\n",
      "1 the Road\n",
      "Automated essay scoring\n",
      "Biomedical text mining\n",
      "Compound term processing\n",
      "Computational linguistics\n",
      "Computer-assisted reviewing\n",
      "Controlled natural language\n",
      "Deep learning\n",
      "Deep linguistic processing\n",
      "Distributional semantics\n",
      "\n",
      "Foreign language reading aid\n",
      "Foreign language writing aid\n",
      "Information extraction\n",
      "Information retrieval\n",
      "Language and Communication Technologies\n",
      "Language technology\n",
      "Latent semantic indexing\n",
      "Native-language identification\n",
      "Natural language programming\n",
      "Natural language search\n",
      "\n",
      "\f",
      "Outline of natural language processing\n",
      "Query expansion\n",
      "Query understanding\n",
      "Reification (linguistics)\n",
      "Speech processing\n",
      "Spoken dialogue system\n",
      "\n",
      "Text-proofing\n",
      "Text simplification\n",
      "Transformer (machine learning model)\n",
      "Truecasing\n",
      "Question answering\n",
      "Word2vec\n",
      "\n",
      "References\n",
      "1. Kongthon, Alisa; Sangkeettrakarn, Chatchawal; Kongyoung, Sarawoot; Haruechaiyasak, Choochart\n",
      "\n",
      "(October 27–30, 2009). Implementing an online help desk system based on conversational agent. MEDES\n",
      "'09: The International Conference on Management of Emergent Digital EcoSystems. France: ACM.\n",
      "doi:10.1145/1643823.1643908 (https://doi.org/10.1145%2F1643823.1643908).\n",
      "\n",
      "2. Hutchins, J. (2005). \"The history of machine translation in a nutshell\" (http://www.hutchinsweb.me.uk/Nutshe\n",
      "\n",
      "ll-2005.pdf) (PDF).\n",
      "\n",
      "3. Koskenniemi, Kimmo (1983), Two-level morphology: A general computational model of word-form\n",
      "\n",
      "recognition and production (http://www.ling.helsinki.fi/~koskenni/doc/Two-LevelMorphology.pdf) (PDF),\n",
      "Department of General Linguistics, University of Helsinki\n",
      "\n",
      "4. Joshi, A. K., & Weinstein, S. (1981, August). Control of Inference: Role of Some Aspects of Discourse\n",
      "Structure-Centering (https://www.ijcai.org/Proceedings/81-1/Papers/071.pdf). In IJCAI (pp. 385-387).\n",
      "\n",
      "5. Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and\n",
      "\n",
      "approaches\". Proceedings of the IEEE. 74 (7): 1026–1035. doi:10.1109/PROC.1986.13580 (https://doi.org/1\n",
      "0.1109%2FPROC.1986.13580). ISSN 1558-2256 (https://www.worldcat.org/issn/1558-2256).\n",
      "S2CID 30688575 (https://api.semanticscholar.org/CorpusID:30688575).\n",
      "\n",
      "6. Chomskyan linguistics encourages the investigation of \"corner cases\" that stress the limits of its theoretical\n",
      "\n",
      "models (comparable to pathological phenomena in mathematics), typically created using thought\n",
      "experiments, rather than the systematic investigation of typical phenomena that occur in real-world data, as\n",
      "is the case in corpus linguistics. The creation and use of such corpora of real-world data is a fundamental\n",
      "part of machine-learning algorithms for natural language processing. In addition, theoretical underpinnings\n",
      "of Chomskyan linguistics such as the so-called \"poverty of the stimulus\" argument entail that general\n",
      "learning algorithms, as are typically used in machine learning, cannot be successful in language\n",
      "processing. As a result, the Chomskyan paradigm discouraged the application of such models to language\n",
      "processing.\n",
      "\n",
      "7. Goldberg, Yoav (2016). \"A Primer on Neural Network Models for Natural Language Processing\". Journal of\n",
      "\n",
      "Artificial Intelligence Research. 57: 345–420. arXiv:1807.10854 (https://arxiv.org/abs/1807.10854).\n",
      "doi:10.1613/jair.4992 (https://doi.org/10.1613%2Fjair.4992). S2CID 8273530 (https://api.semanticscholar.or\n",
      "g/CorpusID:8273530).\n",
      "\n",
      "8. Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016). Deep Learning (http://www.deeplearningbook.or\n",
      "\n",
      "g/). MIT Press.\n",
      "\n",
      "9. Jozefowicz, Rafal; Vinyals, Oriol; Schuster, Mike; Shazeer, Noam; Wu, Yonghui (2016). Exploring the Limits\n",
      "\n",
      "of Language Modeling. arXiv:1602.02410 (https://arxiv.org/abs/1602.02410).\n",
      "Bibcode:2016arXiv160202410J (https://ui.adsabs.harvard.edu/abs/2016arXiv160202410J).\n",
      "\n",
      "10. Choe, Do Kook; Charniak, Eugene. \"Parsing as Language Modeling\" (https://aclanthology.coli.uni-saarland.\n",
      "\n",
      "de/papers/D16-1257/d16-1257). Emnlp 2016.\n",
      "\n",
      "11. Vinyals, Oriol; et al. (2014). \"Grammar as a Foreign Language\" (https://papers.nips.cc/paper/5635-grammar-\n",
      "\n",
      "as-a-foreign-language.pdf) (PDF). Nips2015. arXiv:1412.7449 (https://arxiv.org/abs/1412.7449).\n",
      "Bibcode:2014arXiv1412.7449V (https://ui.adsabs.harvard.edu/abs/2014arXiv1412.7449V).\n",
      "\n",
      "12. Turchin, Alexander; Florez Builes, Luisa F. (2021-03-19). \"Using Natural Language Processing to Measure\n",
      "\n",
      "and Improve Quality of Diabetes Care: A Systematic Review\" (http://journals.sagepub.com/doi/10.1177/1932\n",
      "2968211000831). Journal of Diabetes Science and Technology: 193229682110008.\n",
      "doi:10.1177/19322968211000831 (https://doi.org/10.1177%2F19322968211000831). ISSN 1932-2968 (http\n",
      "s://www.worldcat.org/issn/1932-2968). PMID 33736486 (https://pubmed.ncbi.nlm.nih.gov/33736486).\n",
      "\n",
      "13. Winograd, Terry (1971). Procedures as a Representation for Data in a Computer Program for Understanding\n",
      "\n",
      "Natural Language (http://hci.stanford.edu/winograd/shrdlu/) (Thesis).\n",
      "\n",
      "14. Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into\n",
      "\n",
      "Human Knowledge Structures. Hillsdale: Erlbaum. ISBN 0-470-99033-3.\n",
      "\n",
      "\f",
      "15. Mark Johnson. How the statistical revolution changes (computational) linguistics. (http://www.aclweb.org/ant\n",
      "\n",
      "hology/W09-0103) Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and\n",
      "Computational Linguistics.\n",
      "\n",
      "16. Philip Resnik. Four revolutions. (http://languagelog.ldc.upenn.edu/nll/?p=2946) Language Log, February 5,\n",
      "\n",
      "2011.\n",
      "\n",
      "17. Socher, Richard. \"Deep Learning For NLP-ACL 2012 Tutorial\" (https://www.socher.org/index.php/Main/Deep\n",
      "\n",
      "LearningForNLP-ACL2012Tutorial). www.socher.org. Retrieved 2020-08-17. This was an early Deep\n",
      "Learning tutorial at the ACL 2012 and met with both interest and (at the time) skepticism by most\n",
      "participants. Until then, neural learning was basically rejected because of its lack of statistical interpretability.\n",
      "Until 2015, deep learning had evolved into the major framework of NLP.\n",
      "\n",
      "18. Annamoradnejad, I. (2020). Colbert: Using bert sentence embedding for humor detection (https://arxiv.org/ab\n",
      "\n",
      "s/2004.12765). arXiv preprint arXiv:2004.12765.\n",
      "\n",
      "19. Yi, Chucai; Tian, Yingli (2012), \"Assistive Text Reading from Complex Background for Blind Persons\",\n",
      "\n",
      "Camera-Based Document Analysis and Recognition, Springer Berlin Heidelberg, pp. 15–28,\n",
      "CiteSeerX 10.1.1.668.869 (https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.668.869),\n",
      "doi:10.1007/978-3-642-29364-1_2 (https://doi.org/10.1007%2F978-3-642-29364-1_2),\n",
      "ISBN 9783642293634\n",
      "\n",
      "20. \"What is Natural Language Processing? Intro to NLP in Machine Learning\" (https://www.gyansetu.in/what-is-\n",
      "\n",
      "natural-language-processing/). GyanSetu!. 2020-12-06. Retrieved 2021-01-09.\n",
      "\n",
      "21. Kishorjit, N.; Vidya, Raj RK.; Nirmal, Y.; Sivaji, B. (2012). \"Manipuri Morpheme Identification\" (http://aclweb.o\n",
      "\n",
      "rg/anthology//W/W12/W12-5008.pdf) (PDF). Proceedings of the 3rd Workshop on South and Southeast\n",
      "Asian Natural Language Processing (SANLP). COLING 2012, Mumbai, December 2012: 95–108.\n",
      "\n",
      "22. Klein, Dan; Manning, Christopher D. (2002). \"Natural language grammar induction using a constituent-\n",
      "\n",
      "context model\" (http://papers.nips.cc/paper/1945-natural-language-grammar-induction-using-a-constituent-c\n",
      "ontext-model.pdf) (PDF). Advances in Neural Information Processing Systems.\n",
      "\n",
      "23. PASCAL Recognizing Textual Entailment Challenge (RTE-7) https://tac.nist.gov//2011/RTE/\n",
      "24. Lippi, Marco; Torroni, Paolo (2016-04-20). \"Argumentation Mining: State of the Art and Emerging Trends\" (htt\n",
      "\n",
      "ps://dl.acm.org/doi/10.1145/2850417). ACM Transactions on Internet Technology. 16 (2): 1–25.\n",
      "doi:10.1145/2850417 (https://doi.org/10.1145%2F2850417). ISSN 1533-5399 (https://www.worldcat.org/issn/\n",
      "1533-5399). S2CID 9561587 (https://api.semanticscholar.org/CorpusID:9561587).\n",
      "\n",
      "25. \"Argument Mining - IJCAI2016 Tutorial\" (https://www.i3s.unice.fr/~villata/tutorialIJCAI2016.html).\n",
      "\n",
      "www.i3s.unice.fr. Retrieved 2021-03-09.\n",
      "\n",
      "26. \"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\" (http://acl2016tutorial.arg.tech/).\n",
      "\n",
      "Retrieved 2021-03-09.\n",
      "\n",
      "27. \"U B U W E B :: Racter\" (http://www.ubu.com/historical/racter/index.html). www.ubu.com. Retrieved\n",
      "\n",
      "2020-08-17.\n",
      "\n",
      "28. Writer, Beta (2019). Lithium-Ion Batteries. doi:10.1007/978-3-030-16800-1 (https://doi.org/10.1007%2F978-3\n",
      "\n",
      "-030-16800-1). ISBN 978-3-030-16799-8.\n",
      "\n",
      "29. \"Document Understanding AI on Google Cloud (Cloud Next '19) - YouTube\" (https://www.youtube.com/watc\n",
      "\n",
      "h?v=7dtl650D0y0). www.youtube.com. Retrieved 2021-01-11.\n",
      "\n",
      "30. Administration. \"Centre for Language Technology (CLT)\" (https://www.mq.edu.au/research/research-centres-\n",
      "\n",
      "groups-and-facilities/innovative-technologies/centres/centre-for-language-technology-clt). Macquarie\n",
      "University. Retrieved 2021-01-11.\n",
      "\n",
      "31. \"Shared Task: Grammatical Error Correction\" (https://www.comp.nus.edu.sg/~nlp/conll13st.html).\n",
      "\n",
      "www.comp.nus.edu.sg. Retrieved 2021-01-11.\n",
      "\n",
      "32. \"Shared Task: Grammatical Error Correction\" (https://www.comp.nus.edu.sg/~nlp/conll14st.html).\n",
      "\n",
      "www.comp.nus.edu.sg. Retrieved 2021-01-11.\n",
      "\n",
      "33. \"About Us | Grammarly\" (https://www.grammarly.com/about). www.grammarly.com. Retrieved 2021-01-11.\n",
      "34. Duan, Yucong; Cruz, Christophe (2011). \"Formalizing Semantic of Natural Language through\n",
      "\n",
      "Conceptualization from Existence\" (https://web.archive.org/web/20111009135952/http://www.ijimt.org/abstra\n",
      "ct/100-E00187.htm). International Journal of Innovation, Management and Technology. 2 (1): 37–42.\n",
      "Archived from the original (http://www.ijimt.org/abstract/100-E00187.htm) on 2011-10-09.\n",
      "\n",
      "35. Mittal (2011). \"Versatile question answering systems: seeing in synthesis\" (https://hal.archives-ouvertes.fr/ha\n",
      "\n",
      "l-01104648/file/Mittal_VersatileQA_IJIIDS.pdf) (PDF). International Journal of Intelligent Information and\n",
      "Database Systems. 5 (2): 119–142. doi:10.1504/IJIIDS.2011.038968 (https://doi.org/10.1504%2FIJIIDS.201\n",
      "1.038968).\n",
      "\n",
      "36. \"Previous shared tasks | CoNLL\" (https://www.conll.org/previous-tasks). www.conll.org. Retrieved\n",
      "\n",
      "2021-01-11.\n",
      "\n",
      "\f",
      "37. \"Cognition\" (https://www.lexico.com/definition/cognition). Lexico. Oxford University Press and\n",
      "\n",
      "Dictionary.com. Retrieved 6 May 2020.\n",
      "\n",
      "38. \"Ask the Cognitive Scientist\" (http://www.aft.org/newspubs/periodicals/ae/summer2002/willingham.cfm).\n",
      "\n",
      "American Federation of Teachers. 8 August 2014. \"Cognitive science is an interdisciplinary field of\n",
      "researchers from Linguistics, psychology, neuroscience, philosophy, computer science, and anthropology\n",
      "that seek to understand the mind.\"\n",
      "\n",
      "39. Robinson, Peter (2008). Handbook of Cognitive Linguistics and Second Language Acquisition. Routledge.\n",
      "\n",
      "pp. 3–8. ISBN 978-0-805-85352-0.\n",
      "\n",
      "40. Lakoff, George (1999). Philosophy in the Flesh: The Embodied Mind and Its Challenge to Western\n",
      "\n",
      "Philosophy; Appendix: The Neural Theory of Language Paradigm. New York Basic Books. pp. 569–583.\n",
      "ISBN 978-0-465-05674-3.\n",
      "\n",
      "41. Strauss, Claudia (1999). A Cognitive Theory of Cultural Meaning. Cambridge University Press. pp. 156–\n",
      "\n",
      "164. ISBN 978-0-521-59541-4.\n",
      "\n",
      "42. \"Universal Conceptual Cognitive Annotation (UCCA)\" (https://universalconceptualcognitiveannotation.githu\n",
      "\n",
      "b.io/). Universal Conceptual Cognitive Annotation (UCCA). Retrieved 2021-01-11.\n",
      "\n",
      "43. Rodríguez, F. C., & Mairal-Usón, R. (2016). Building an RRG computational grammar (https://www.redalyc.or\n",
      "\n",
      "g/pdf/1345/134549291020.pdf). Onomazein, (34), 86-117.\n",
      "\n",
      "44. \"Fluid Construction Grammar – A fully operational processing system for construction grammars\" (https://ww\n",
      "\n",
      "w.fcg-net.org/). Retrieved 2021-01-11.\n",
      "\n",
      "45. \"ACL Member Portal | The Association for Computational Linguistics Member Portal\" (https://www.aclweb.or\n",
      "\n",
      "g/portal/). www.aclweb.org. Retrieved 2021-01-11.\n",
      "\n",
      "46. \"Chunks and Rules\" (https://www.w3.org/Data/demos/chunks/chunks.html). www.w3.org. Retrieved\n",
      "\n",
      "2021-01-11.\n",
      "\n",
      "47. Socher, Richard; Karpathy, Andrej; Le, Quoc V.; Manning, Christopher D.; Ng, Andrew Y. (2014). \"Grounded\n",
      "Compositional Semantics for Finding and Describing Images with Sentences\" (https://www.mitpressjournal\n",
      "s.org/doi/pdfplus/10.1162/tacl_a_00177). Transactions of the Association for Computational Linguistics. 2:\n",
      "207–218. doi:10.1162/tacl_a_00177 (https://doi.org/10.1162%2Ftacl_a_00177). S2CID 2317858 (https://api.\n",
      "semanticscholar.org/CorpusID:2317858).\n",
      "\n",
      "Further reading\n",
      "\n",
      "Bates, M (1995). \"Models of natural language understanding\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PM\n",
      "C40721). Proceedings of the National Academy of Sciences of the United States of America. 92 (22): 9977–\n",
      "9982. Bibcode:1995PNAS...92.9977B (https://ui.adsabs.harvard.edu/abs/1995PNAS...92.9977B).\n",
      "doi:10.1073/pnas.92.22.9977 (https://doi.org/10.1073%2Fpnas.92.22.9977). PMC 40721 (https://www.ncbi.n\n",
      "lm.nih.gov/pmc/articles/PMC40721). PMID 7479812 (https://pubmed.ncbi.nlm.nih.gov/7479812).\n",
      "Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly\n",
      "Media. ISBN 978-0-596-51649-9.\n",
      "Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson\n",
      "Prentice Hall. ISBN 978-0-13-187321-6.\n",
      "Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech,\n",
      "morphology, and syntax, Volume 1. ISTE-Wiley. ISBN 978-1848218482.\n",
      "Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics,\n",
      "discourse, and applications, Volume 2. ISTE-Wiley. ISBN 978-1848219212.\n",
      "Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information\n",
      "Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5. Official html and pdf versions available\n",
      "without charge. (http://nlp.stanford.edu/IR-book/)\n",
      "Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language\n",
      "Processing. The MIT Press. ISBN 978-0-262-13360-9.\n",
      "David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-\n",
      "Verlag. ISBN 978-0-387-19557-5.\n",
      "\n",
      "Retrieved from \"https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1020063620\"\n",
      "\n",
      "This page was last edited on 26 April 2021, at 23:57 (UTC).\n",
      "\n",
      "\f",
      "Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree\n",
      "to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit\n",
      "organization.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "output_string = StringIO()\n",
    "with open('file/Natural_language_processing.pdf', 'rb') as in_file:\n",
    "    parser = PDFParser(in_file)\n",
    "    doc = PDFDocument(parser)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    for page in PDFPage.create_pages(doc):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "print(output_string.getvalue())\n",
    "\n",
    "in_file.close()\n",
    "device.close()\n",
    "output_string.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **3. HTML** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Natural language processing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   document.documentElement.className=\"client-js\";RLCONF={\"wgBreakFrames\":!1,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"22826073-f4ba-4901-b86d-d9aa3f82643c\",\"wgCSPNonce\":!1,\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":!1,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1020063620,\"wgRevisionId\":1020063620,\"wgArticleId\":21652,\"wgIsArticle\":!0,\"wgIsRedirect\":!1,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"CS1 maint: location\",\"Articles with short description\",\"Short description matches Wikidata\",\"Commons link fro\n"
     ]
    }
   ],
   "source": [
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()\n",
    "# Parse the HTML file\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "print (strhtm[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Jump to navigation\n",
      "Jump to search\n",
      "None\n",
      "None\n",
      "automated online assistant\n",
      "customer service\n",
      "[1]\n",
      "linguistics\n",
      "computer science\n",
      "artificial intelligence\n",
      "natural language\n",
      "speech recognition\n",
      "natural language understanding\n",
      "natural-language generation\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "edit\n",
      "History of natural language processing\n",
      "Alan Turing\n",
      "Computing Machinery and Intelligence\n",
      "Turing test\n",
      "edit\n",
      "John Searle\n",
      "Chinese room\n",
      "Georgetown experiment\n",
      "automatic translation\n",
      "[2]\n",
      "ALPAC report\n",
      "statistical machine translation\n",
      "SHRDLU\n",
      "blocks worlds\n",
      "ELIZA\n",
      "Rogerian psychotherapist\n",
      "Joseph Weizenbaum\n",
      "ontologies\n",
      "chatterbots\n",
      "PARRY\n",
      "HPSG\n",
      "generative grammar\n",
      "[3]\n",
      "Lesk algorithm\n",
      "[4]\n",
      "Rhetorical Structure Theory\n",
      "Racter\n",
      "Jabberwacky\n",
      "[5]\n",
      "edit\n",
      "machine learning\n",
      "Moore's law\n",
      "Chomskyan\n",
      "transformational grammar\n",
      "corpus linguistics\n",
      "[6]\n",
      "machine translation\n",
      "textual corpora\n",
      "Parliament of Canada\n",
      "European Union\n",
      "unsupervised\n",
      "semi-supervised learning\n",
      "supervised learning\n",
      "World Wide Web\n",
      "time complexity\n",
      "edit\n",
      "representation learning\n",
      "deep neural network\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "edit\n",
      "[13]\n",
      "[14]\n",
      "stemming\n",
      "machine-learning\n",
      "Apertium\n",
      "tokenization\n",
      "knowledge extraction\n",
      "edit\n",
      "[15]\n",
      "[16]\n",
      "statistical inference\n",
      "corpora\n",
      "statistical models\n",
      "probabilistic\n",
      "real-valued\n",
      "decision trees\n",
      "part-of-speech tagging\n",
      "hidden Markov models\n",
      "statistical models\n",
      "probabilistic\n",
      "real-valued\n",
      "cache language models\n",
      "speech recognition\n",
      "edit\n",
      "Artificial neural network\n",
      "[17]\n",
      "neural networks\n",
      "word embeddings\n",
      "neural machine translation\n",
      "sequence-to-sequence\n",
      "statistical machine translation\n",
      "[18]\n",
      "edit\n",
      "edit\n",
      "Optical character recognition\n",
      "Speech recognition\n",
      "text to speech\n",
      "AI-complete\n",
      "natural speech\n",
      "speech segmentation\n",
      "coarticulation\n",
      "analog signal\n",
      "Speech segmentation\n",
      "speech recognition\n",
      "Text-to-speech\n",
      "[19]\n",
      "Word segmentation\n",
      "Tokenization\n",
      "English\n",
      "Chinese\n",
      "Japanese\n",
      "Thai\n",
      "vocabulary\n",
      "morphology\n",
      "bag of words\n",
      "edit\n",
      "Lemmatization\n",
      "[20]\n",
      "Morphological segmentation\n",
      "morphemes\n",
      "morphology\n",
      "English\n",
      "inflectional morphology\n",
      "Turkish\n",
      "Meitei\n",
      "[21]\n",
      "agglutinated\n",
      "Part-of-speech tagging\n",
      "part of speech\n",
      "parts of speech\n",
      "noun\n",
      "verb\n",
      "noun\n",
      "verb\n",
      "adjective\n",
      "Stemming\n",
      "edit\n",
      "Grammar induction\n",
      "[22]\n",
      "formal grammar\n",
      "Sentence breaking\n",
      "sentence boundary disambiguation\n",
      "periods\n",
      "punctuation marks\n",
      "abbreviations\n",
      "Parsing\n",
      "parse tree\n",
      "grammar\n",
      "natural languages\n",
      "ambiguous\n",
      "probabilistic context-free grammar\n",
      "stochastic grammar\n",
      "edit\n",
      "Lexical semantics\n",
      "Distributional semantics\n",
      "Named entity recognition\n",
      "capitalization\n",
      "named entity\n",
      "Chinese\n",
      "Arabic\n",
      "German\n",
      "nouns\n",
      "French\n",
      "Spanish\n",
      "adjectives\n",
      "Sentiment analysis\n",
      "Multimodal sentiment analysis\n",
      "Terminology extraction\n",
      "Word sense disambiguation\n",
      "meaning\n",
      "WordNet\n",
      "edit\n",
      "Relationship extraction\n",
      "Semantic parsing\n",
      "AMR parsing\n",
      "DRT parsing\n",
      "Natural language understanding\n",
      "Semantic role labelling\n",
      "frames\n",
      "semantic roles\n",
      "edit\n",
      "Coreference resolution\n",
      "Anaphora resolution\n",
      "pronouns\n",
      "referring expressions\n",
      "Discourse analysis\n",
      "discourse\n",
      "speech acts\n",
      "frames\n",
      "Semantic role labelling\n",
      "pro-drop languages\n",
      "Recognizing textual entailment\n",
      "[23]\n",
      "Topic segmentation\n",
      "Argument mining\n",
      "natural language\n",
      "[24]\n",
      "argument scheme\n",
      "[25]\n",
      "[26]\n",
      "edit\n",
      "Automatic summarization\n",
      "[27]\n",
      "1 the Road\n",
      "language models\n",
      "[28]\n",
      "Dialogue management\n",
      "Document AI\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "GPT-2\n",
      "[33]\n",
      "Machine translation\n",
      "AI-complete\n",
      "Natural language generation\n",
      "Natural language understanding\n",
      "first-order logic\n",
      "computer\n",
      "closed-world assumption\n",
      "open-world assumption\n",
      "[34]\n",
      "Question answering\n",
      "[35]\n",
      "edit\n",
      "[36]\n",
      "edit\n",
      "Cognition\n",
      "[37]\n",
      "Cognitive science\n",
      "[38]\n",
      "Cognitive linguistics\n",
      "[39]\n",
      "symbolic NLP\n",
      "George Lakoff\n",
      "cognitive science\n",
      "cognitive linguistics\n",
      "[40]\n",
      "conceptual metaphor\n",
      "[41]\n",
      "probabilistic context-free grammar\n",
      "US patent 9269353\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "ACT-R\n",
      "[45]\n",
      "ACL\n",
      "explainability\n",
      "[46]\n",
      "[47]\n",
      "edit\n",
      "1 the Road\n",
      "Automated essay scoring\n",
      "Biomedical text mining\n",
      "Compound term processing\n",
      "Computational linguistics\n",
      "Computer-assisted reviewing\n",
      "Controlled natural language\n",
      "Deep learning\n",
      "Deep linguistic processing\n",
      "Distributional semantics\n",
      "Foreign language reading aid\n",
      "Foreign language writing aid\n",
      "Information extraction\n",
      "Information retrieval\n",
      "Language and Communication Technologies\n",
      "Language technology\n",
      "Latent semantic indexing\n",
      "Native-language identification\n",
      "Natural language programming\n",
      "Natural language search\n",
      "Outline of natural language processing\n",
      "Query expansion\n",
      "Query understanding\n",
      "Reification (linguistics)\n",
      "Speech processing\n",
      "Spoken dialogue system\n",
      "Text-proofing\n",
      "Text simplification\n",
      "Transformer (machine learning model)\n",
      "Truecasing\n",
      "Question answering\n",
      "Word2vec\n",
      "edit\n",
      "^\n",
      "doi\n",
      "10.1145/1643823.1643908\n",
      "^\n",
      "\"The history of machine translation in a nutshell\"\n",
      "self-published source\n",
      "^\n",
      "Koskenniemi, Kimmo\n",
      "Two-level morphology: A general computational model of word-form recognition and production\n",
      "University of Helsinki\n",
      "^\n",
      "Control of Inference: Role of Some Aspects of Discourse Structure-Centering\n",
      "^\n",
      "doi\n",
      "10.1109/PROC.1986.13580\n",
      "ISSN\n",
      "1558-2256\n",
      "S2CID\n",
      "30688575\n",
      "^\n",
      "corner cases\n",
      "pathological\n",
      "thought experiments\n",
      "corpus linguistics\n",
      "corpora\n",
      "poverty of the stimulus\n",
      "^\n",
      "arXiv\n",
      "1807.10854\n",
      "doi\n",
      "10.1613/jair.4992\n",
      "S2CID\n",
      "8273530\n",
      "^\n",
      "Deep Learning\n",
      "^\n",
      "arXiv\n",
      "1602.02410\n",
      "Bibcode\n",
      "2016arXiv160202410J\n",
      "^\n",
      "\"Parsing as Language Modeling\"\n",
      "^\n",
      "\"Grammar as a Foreign Language\"\n",
      "arXiv\n",
      "1412.7449\n",
      "Bibcode\n",
      "2014arXiv1412.7449V\n",
      "^\n",
      "\"Using Natural Language Processing to Measure and Improve Quality of Diabetes Care: A Systematic Review\"\n",
      "doi\n",
      "10.1177/19322968211000831\n",
      "ISSN\n",
      "1932-2968\n",
      "PMID\n",
      "33736486\n",
      "^\n",
      "Procedures as a Representation for Data in a Computer Program for Understanding Natural Language\n",
      "^\n",
      "ISBN\n",
      "0-470-99033-3\n",
      "^\n",
      "Mark Johnson. How the statistical revolution changes (computational) linguistics.\n",
      "^\n",
      "Philip Resnik. Four revolutions.\n",
      "^\n",
      "\"Deep Learning For NLP-ACL 2012 Tutorial\"\n",
      "^\n",
      "Colbert: Using bert sentence embedding for humor detection\n",
      "^\n",
      "CiteSeerX\n",
      "10.1.1.668.869\n",
      "doi\n",
      "10.1007/978-3-642-29364-1_2\n",
      "ISBN\n",
      "9783642293634\n",
      "^\n",
      "\"What is Natural Language Processing? Intro to NLP in Machine Learning\"\n",
      "^\n",
      "\"Manipuri Morpheme Identification\"\n",
      "link\n",
      "^\n",
      "\"Natural language grammar induction using a constituent-context model\"\n",
      "^\n",
      "https://tac.nist.gov//2011/RTE/\n",
      "^\n",
      "\"Argumentation Mining: State of the Art and Emerging Trends\"\n",
      "doi\n",
      "10.1145/2850417\n",
      "ISSN\n",
      "1533-5399\n",
      "S2CID\n",
      "9561587\n",
      "^\n",
      "\"Argument Mining - IJCAI2016 Tutorial\"\n",
      "^\n",
      "\"NLP Approaches to Computational Argumentation – ACL 2016, Berlin\"\n",
      "^\n",
      "\"U B U W E B :: Racter\"\n",
      "^\n",
      "doi\n",
      "10.1007/978-3-030-16800-1\n",
      "ISBN\n",
      "978-3-030-16799-8\n",
      "^\n",
      "\"Document Understanding AI on Google Cloud (Cloud Next '19) - YouTube\"\n",
      "^\n",
      "\"Centre for Language Technology (CLT)\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"Shared Task: Grammatical Error Correction\"\n",
      "^\n",
      "\"About Us | Grammarly\"\n",
      "^\n",
      "\"Formalizing Semantic of Natural Language through Conceptualization from Existence\"\n",
      "the original\n",
      "^\n",
      "\"Versatile question answering systems: seeing in synthesis\"\n",
      "doi\n",
      "10.1504/IJIIDS.2011.038968\n",
      "^\n",
      "\"Previous shared tasks | CoNLL\"\n",
      "^\n",
      "\"Cognition\"\n",
      "Oxford University Press\n",
      "Dictionary.com\n",
      "^\n",
      "\"Ask the Cognitive Scientist\"\n",
      "^\n",
      "ISBN\n",
      "978-0-805-85352-0\n",
      "^\n",
      "ISBN\n",
      "978-0-465-05674-3\n",
      "^\n",
      "ISBN\n",
      "978-0-521-59541-4\n",
      "^\n",
      "\"Universal Conceptual Cognitive Annotation (UCCA)\"\n",
      "^\n",
      "Building an RRG computational grammar\n",
      "^\n",
      "\"Fluid Construction Grammar – A fully operational processing system for construction grammars\"\n",
      "^\n",
      "\"ACL Member Portal | The Association for Computational Linguistics Member Portal\"\n",
      "^\n",
      "\"Chunks and Rules\"\n",
      "^\n",
      "\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\"\n",
      "doi\n",
      "10.1162/tacl_a_00177\n",
      "S2CID\n",
      "2317858\n",
      "edit\n",
      "\"Models of natural language understanding\"\n",
      "Bibcode\n",
      "1995PNAS...92.9977B\n",
      "doi\n",
      "10.1073/pnas.92.22.9977\n",
      "PMC\n",
      "40721\n",
      "PMID\n",
      "7479812\n",
      "ISBN\n",
      "978-0-596-51649-9\n",
      "ISBN\n",
      "978-0-13-187321-6\n",
      "ISBN\n",
      "978-1848218482\n",
      "ISBN\n",
      "978-1848219212\n",
      "ISBN\n",
      "978-0-521-86571-5\n",
      "Official html and pdf versions available without charge.\n",
      "ISBN\n",
      "978-0-262-13360-9\n",
      "ISBN\n",
      "978-0-387-19557-5\n",
      "Natural language processing\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "AI-complete\n",
      "Bag-of-words\n",
      "n-gram\n",
      "Bigram\n",
      "Trigram\n",
      "Natural language understanding\n",
      "Speech corpus\n",
      "Stopwords\n",
      "Text corpus\n",
      "Text analysis\n",
      "Collocation extraction\n",
      "Concept mining\n",
      "Compound term processing\n",
      "Coreference resolution\n",
      "Lemmatisation\n",
      "Named-entity recognition\n",
      "Ontology learning\n",
      "Parsing\n",
      "Part-of-speech tagging\n",
      "Semantic similarity\n",
      "Sentiment analysis\n",
      "Stemming\n",
      "Terminology extraction\n",
      "Text chunking\n",
      "Text segmentation\n",
      "Sentence segmentation\n",
      "Word segmentation\n",
      "Textual entailment\n",
      "Truecasing\n",
      "Word-sense disambiguation\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Neural\n",
      "None\n",
      "Speech recognition\n",
      "Speech segmentation\n",
      "Speech synthesis\n",
      "Natural language generation\n",
      "Optical character recognition\n",
      "Topic model\n",
      "Latent Dirichlet allocation\n",
      "Latent semantic analysis\n",
      "Pachinko allocation\n",
      "None\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Spell checker\n",
      "Syntax guessing\n",
      "None\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Question answering\n",
      "Virtual assistant\n",
      "Voice user interface\n",
      "Authority control\n",
      "None\n",
      "LCCN\n",
      "sh88002425\n",
      "NDL\n",
      "00562347\n",
      "None\n",
      "Language portal\n",
      "https://en.wikipedia.org/w/index.php?title=Natural_language_processing&oldid=1020063620\n",
      "Categories\n",
      "Natural language processing\n",
      "Computational linguistics\n",
      "Speech recognition\n",
      "Computational fields of study\n",
      "Artificial intelligence\n",
      "CS1 maint: location\n",
      "Articles with short description\n",
      "Short description matches Wikidata\n",
      "Commons link from Wikidata\n",
      "Wikipedia articles with LCCN identifiers\n",
      "Wikipedia articles with NDL identifiers\n",
      "Talk\n",
      "Contributions\n",
      "Create account\n",
      "Log in\n",
      "Article\n",
      "Talk\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "None\n",
      "Main page\n",
      "Contents\n",
      "Current events\n",
      "Random article\n",
      "About Wikipedia\n",
      "Contact us\n",
      "Donate\n",
      "Help\n",
      "Learn to edit\n",
      "Community portal\n",
      "Recent changes\n",
      "Upload file\n",
      "What links here\n",
      "Related changes\n",
      "Upload file\n",
      "Special pages\n",
      "Permanent link\n",
      "Page information\n",
      "Cite this page\n",
      "Wikidata item\n",
      "Download as PDF\n",
      "Printable version\n",
      "Wikimedia Commons\n",
      "Afrikaans\n",
      "العربية\n",
      "Azərbaycanca\n",
      "বাংলা\n",
      "Bân-lâm-gú\n",
      "Беларуская\n",
      "Беларуская (тарашкевіца)‎\n",
      "Български\n",
      "Català\n",
      "Čeština\n",
      "Dansk\n",
      "Deutsch\n",
      "Eesti\n",
      "Ελληνικά\n",
      "Español\n",
      "Euskara\n",
      "فارسی\n",
      "Français\n",
      "Galego\n",
      "한국어\n",
      "Հայերեն\n",
      "हिन्दी\n",
      "Hrvatski\n",
      "Bahasa Indonesia\n",
      "Íslenska\n",
      "Italiano\n",
      "עברית\n",
      "ಕನ್ನಡ\n",
      "ქართული\n",
      "Lietuvių\n",
      "Македонски\n",
      "मराठी\n",
      "Монгол\n",
      "မြန်မာဘာသာ\n",
      "日本語\n",
      "ଓଡ଼ିଆ\n",
      "Piemontèis\n",
      "Polski\n",
      "Português\n",
      "Română\n",
      "Русский\n",
      "Simple English\n",
      "کوردی\n",
      "Српски / srpski\n",
      "Srpskohrvatski / српскохрватски\n",
      "தமிழ்\n",
      "ไทย\n",
      "Türkçe\n",
      "Українська\n",
      "Tiếng Việt\n",
      "粵語\n",
      "中文\n",
      "Edit links\n",
      "Creative Commons Attribution-ShareAlike License\n",
      "None\n",
      "Terms of Use\n",
      "Privacy Policy\n",
      "Wikimedia Foundation, Inc.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Mobile view\n",
      "Developers\n",
      "Statistics\n",
      "Cookie statement\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Extracting all instances of a particular tag\n",
    "for x in soup.find_all('a'): print(x.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
      "\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.\n",
      "\n",
      "Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\n",
      "\n",
      "The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.\n",
      "\n",
      "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[6]\n",
      "\n",
      "In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[7][8] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[9] parsing,[10][11] and many others. This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.[12]\n",
      "\n",
      "In the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup:[13][14] such as by writing grammars or devising heuristic rules for stemming.\n",
      "\n",
      "More recent systems based on machine-learning algorithms have many advantages over hand-produced rules: \n",
      "\n",
      "Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used\n",
      "\n",
      "Since the so-called \"statistical revolution\"[15][16] in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\n",
      "\n",
      "Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "Since the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\n",
      "\n",
      "A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[17] the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.[18]\n",
      "\n",
      "The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n",
      "\n",
      "Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n",
      "\n",
      "Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[36]\n",
      "\n",
      "Most more higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n",
      "\n",
      "Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[37] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[38] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[39] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies. \n",
      "\n",
      "As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[40] with two defining aspects: \n",
      "\n",
      "Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[42] functional grammar,[43] construction grammar,[44] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[45] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[46] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).[47]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting all text of a particular tag\n",
    "wiki = ''\n",
    "for x in soup.find_all('p'): wiki += x.text + '\\n'\n",
    "print(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Scraping the text from the web is out of my scope. Many books introduce the techniques on this topic. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font size=\"7\"> Part 2 Processing the Text </font> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **1. Sentence tokenization**</font>  <br>\n",
    "<font size=\"4\"> The process of splitting a text corpus into sentences that act as the first level of tokens the corpus is comprised of. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # natural language toolkit\n",
    "default_st = nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sentences = default_st(text=wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.',\n",
       " 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.',\n",
       " 'Natural language processing has its roots in the 1950s.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 sentences\n",
    "wiki_sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\">\n",
    "It doesn’t just use periods to delimit sentences, but also considers other punctuation and capitalization of words.\n",
    "</font>  \n",
    "* <font size=\"4\">\n",
    "It supports many lanuages.\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .',\n",
       " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .',\n",
       " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .',\n",
       " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .',\n",
       " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import some german texts\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "german_sentences = default_st(text=german_text, language='german')\n",
    "german_sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> Otherwise, we could also use specific  <span style=\"color:blue\"> **regular expression-based** </span> patterns to segment sentences. To this end, the *RegexpTokenizer* class is helpful.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "wiki_sentences = regex_st.tokenize(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips\n",
    "* (?<!...)\n",
    "Matches if the current position in the string is not preceded by a match for .... This is called a negative lookbehind assertion. Similar to positive lookbehind assertions, the contained pattern must only match strings of some fixed length. Patterns which start with negative lookbehind assertions may match at the beginning of the string being searched.\n",
    "* (?<=...)\n",
    "Matches if the current position in the string is preceded by a match for ... that ends at the current position. This is called a positive lookbehind assertion. (?<=abc)def will find a match in 'abcdef', since the lookbehind will back up 3 characters and check if the contained pattern matches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " ' The result is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.',\n",
       " '\\n\\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural-language generation.',\n",
       " '\\nNatural language processing has its roots in the 1950s.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sentences[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **2. Word tokenization**</font>  <br>\n",
    "<font size=\"4\"> The process of splitting or segmenting sentences into their constituent words. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " ',',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'result',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'understanding',\n",
       " \"''\",\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " '.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " '.',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'and',\n",
       " 'natural-language',\n",
       " 'generation',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'has',\n",
       " 'its',\n",
       " 'roots',\n",
       " 'in',\n",
       " 'the',\n",
       " '1950s',\n",
       " '.',\n",
       " 'Already',\n",
       " 'in',\n",
       " '1950',\n",
       " ',',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'published',\n",
       " 'an',\n",
       " 'article',\n",
       " 'titled',\n",
       " '``',\n",
       " 'Computing',\n",
       " 'Machinery',\n",
       " 'and',\n",
       " 'Intelligence',\n",
       " \"''\",\n",
       " 'which',\n",
       " 'proposed',\n",
       " 'what',\n",
       " 'is',\n",
       " 'now',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Turing',\n",
       " 'test',\n",
       " 'as',\n",
       " 'a',\n",
       " 'criterion',\n",
       " 'of',\n",
       " 'intelligence',\n",
       " ',',\n",
       " 'a',\n",
       " 'task',\n",
       " 'that',\n",
       " 'involves',\n",
       " 'the',\n",
       " 'automated',\n",
       " 'interpretation',\n",
       " 'and',\n",
       " 'generation',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " ',',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'not',\n",
       " 'articulated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '.',\n",
       " 'The',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'symbolic',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'well-summarized',\n",
       " 'by',\n",
       " 'John',\n",
       " 'Searle',\n",
       " \"'s\",\n",
       " 'Chinese',\n",
       " 'room',\n",
       " 'experiment',\n",
       " ':',\n",
       " 'Given',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'rules',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'a',\n",
       " 'Chinese',\n",
       " 'phrasebook',\n",
       " ',',\n",
       " 'with',\n",
       " 'questions',\n",
       " 'and',\n",
       " 'matching',\n",
       " 'answers',\n",
       " ')',\n",
       " ',',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '(',\n",
       " 'or',\n",
       " 'other',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " ')',\n",
       " 'by',\n",
       " 'applying',\n",
       " 'those',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'the',\n",
       " 'data',\n",
       " 'it',\n",
       " 'is',\n",
       " 'confronted',\n",
       " 'with',\n",
       " '.',\n",
       " 'Up',\n",
       " 'to',\n",
       " 'the',\n",
       " '1980s',\n",
       " ',',\n",
       " 'most',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'based',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'sets',\n",
       " 'of',\n",
       " 'hand-written',\n",
       " 'rules',\n",
       " '.',\n",
       " 'Starting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'for',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'This',\n",
       " 'was',\n",
       " 'due',\n",
       " 'to',\n",
       " 'both',\n",
       " 'the',\n",
       " 'steady',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'power',\n",
       " '(',\n",
       " 'see',\n",
       " 'Moore',\n",
       " \"'s\",\n",
       " 'law',\n",
       " ')',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gradual',\n",
       " 'lessening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dominance',\n",
       " 'of',\n",
       " 'Chomskyan',\n",
       " 'theories',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " '(',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'transformational',\n",
       " 'grammar',\n",
       " ')',\n",
       " ',',\n",
       " 'whose',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'discouraged',\n",
       " 'the',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'that',\n",
       " 'underlies',\n",
       " 'the',\n",
       " 'machine-learning',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " '[',\n",
       " '6',\n",
       " ']',\n",
       " 'In',\n",
       " 'the',\n",
       " '2010s',\n",
       " ',',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-style',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'methods',\n",
       " 'became',\n",
       " 'widespread',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " ',',\n",
       " 'due',\n",
       " 'in',\n",
       " 'part',\n",
       " 'to',\n",
       " 'a',\n",
       " 'flurry',\n",
       " 'of',\n",
       " 'results',\n",
       " 'showing',\n",
       " 'that',\n",
       " 'such',\n",
       " 'techniques',\n",
       " '[',\n",
       " '7',\n",
       " ']',\n",
       " '[',\n",
       " '8',\n",
       " ']',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'state-of-the-art',\n",
       " 'results',\n",
       " 'in',\n",
       " 'many',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'for',\n",
       " 'example',\n",
       " 'in',\n",
       " 'language',\n",
       " 'modeling',\n",
       " ',',\n",
       " '[',\n",
       " '9',\n",
       " ']',\n",
       " 'parsing',\n",
       " ',',\n",
       " '[',\n",
       " '10',\n",
       " ']',\n",
       " '[',\n",
       " '11',\n",
       " ']',\n",
       " 'and',\n",
       " 'many',\n",
       " 'others',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'increasingly',\n",
       " 'important',\n",
       " 'in',\n",
       " 'medicine',\n",
       " 'and',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'where',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'being',\n",
       " 'used',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'notes',\n",
       " 'and',\n",
       " 'text',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'health',\n",
       " 'records',\n",
       " 'that',\n",
       " 'would',\n",
       " 'otherwise',\n",
       " 'be',\n",
       " 'inaccessible',\n",
       " 'for',\n",
       " 'study',\n",
       " 'when',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'care',\n",
       " '.',\n",
       " '[',\n",
       " '12',\n",
       " ']',\n",
       " 'In',\n",
       " 'the',\n",
       " 'early',\n",
       " 'days',\n",
       " ',',\n",
       " 'many',\n",
       " 'language-processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'designed',\n",
       " 'by',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " ',',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'the',\n",
       " 'hand-coding',\n",
       " 'of',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'rules',\n",
       " ',',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'lookup',\n",
       " ':',\n",
       " '[',\n",
       " '13',\n",
       " ']',\n",
       " '[',\n",
       " '14',\n",
       " ']',\n",
       " 'such',\n",
       " 'as',\n",
       " 'by',\n",
       " 'writing',\n",
       " 'grammars',\n",
       " 'or',\n",
       " 'devising',\n",
       " 'heuristic',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'stemming',\n",
       " '.',\n",
       " 'More',\n",
       " 'recent',\n",
       " 'systems',\n",
       " 'based',\n",
       " 'on',\n",
       " 'machine-learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'many',\n",
       " 'advantages',\n",
       " 'over',\n",
       " 'hand-produced',\n",
       " 'rules',\n",
       " ':',\n",
       " 'Despite',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research',\n",
       " ',',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " 'are',\n",
       " 'still',\n",
       " '(',\n",
       " '2020',\n",
       " ')',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'so-called',\n",
       " '``',\n",
       " 'statistical',\n",
       " 'revolution',\n",
       " \"''\",\n",
       " '[',\n",
       " '15',\n",
       " ']',\n",
       " '[',\n",
       " '16',\n",
       " ']',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'and',\n",
       " 'mid-1990s',\n",
       " ',',\n",
       " 'much',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'research',\n",
       " 'has',\n",
       " 'relied',\n",
       " 'heavily',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'using',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rules',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'large',\n",
       " 'corpora',\n",
       " '(',\n",
       " 'the',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'of',\n",
       " 'corpus',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'possibly',\n",
       " 'with',\n",
       " 'human',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'annotations',\n",
       " ')',\n",
       " 'of',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'examples',\n",
       " '.',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'classes',\n",
       " 'of',\n",
       " 'machine-learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'natural-language-processing',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'These',\n",
       " 'algorithms',\n",
       " 'take',\n",
       " 'as',\n",
       " 'input',\n",
       " 'a',\n",
       " 'large',\n",
       " 'set',\n",
       " 'of',\n",
       " '``',\n",
       " 'features',\n",
       " \"''\",\n",
       " 'that',\n",
       " 'are',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'Increasingly',\n",
       " ',',\n",
       " 'however',\n",
       " ',',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models',\n",
       " ',',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " ',',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'each',\n",
       " 'input',\n",
       " 'feature',\n",
       " '.',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'have',\n",
       " 'the',\n",
       " 'advantage',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'express',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'certainty',\n",
       " 'of',\n",
       " 'many',\n",
       " 'different',\n",
       " 'possible',\n",
       " 'answers',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'only',\n",
       " 'one',\n",
       " ',',\n",
       " 'producing',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'such',\n",
       " 'a',\n",
       " 'model',\n",
       " 'is',\n",
       " 'included',\n",
       " 'as',\n",
       " 'a',\n",
       " 'component',\n",
       " 'of',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " '.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earliest-used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'decision',\n",
       " 'trees',\n",
       " ',',\n",
       " 'produced',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'hard',\n",
       " 'if-then',\n",
       " 'rules',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'existing',\n",
       " 'hand-written',\n",
       " 'rules',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'introduced',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'hidden',\n",
       " 'Markov',\n",
       " 'models',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " ',',\n",
       " 'and',\n",
       " 'increasingly',\n",
       " ',',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models',\n",
       " ',',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " ',',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'the',\n",
       " 'features',\n",
       " 'making',\n",
       " 'up',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'cache',\n",
       " 'language',\n",
       " 'models',\n",
       " 'upon',\n",
       " 'which',\n",
       " 'many',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'systems',\n",
       " 'now',\n",
       " 'rely',\n",
       " 'are',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'such',\n",
       " 'statistical',\n",
       " 'models',\n",
       " '.',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'more',\n",
       " 'robust',\n",
       " 'when',\n",
       " 'given',\n",
       " 'unfamiliar',\n",
       " 'input',\n",
       " ',',\n",
       " 'especially',\n",
       " 'input',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'errors',\n",
       " '(',\n",
       " 'as',\n",
       " 'is',\n",
       " 'very',\n",
       " 'common',\n",
       " 'for',\n",
       " 'real-world',\n",
       " 'data',\n",
       " ')',\n",
       " ',',\n",
       " 'and',\n",
       " 'produce',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'integrated',\n",
       " 'into',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " 'comprising',\n",
       " 'multiple',\n",
       " 'subtasks',\n",
       " '.',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'neural',\n",
       " 'turn',\n",
       " ',',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research',\n",
       " 'have',\n",
       " 'been',\n",
       " 'largely',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'neural',\n",
       " 'networks',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'they',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'be',\n",
       " 'relevant',\n",
       " 'for',\n",
       " 'contexts',\n",
       " 'in',\n",
       " 'which',\n",
       " 'statistical',\n",
       " 'interpretability',\n",
       " 'and',\n",
       " 'transparency',\n",
       " 'is',\n",
       " 'required',\n",
       " '.',\n",
       " 'A',\n",
       " 'major',\n",
       " 'drawback',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'require',\n",
       " 'elaborate',\n",
       " 'feature',\n",
       " 'engineering',\n",
       " '.',\n",
       " 'Since',\n",
       " '2015',\n",
       " ',',\n",
       " '[',\n",
       " '17',\n",
       " ']',\n",
       " 'the',\n",
       " 'field',\n",
       " 'has',\n",
       " 'thus',\n",
       " 'largely',\n",
       " 'abandoned',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'and',\n",
       " 'shifted',\n",
       " 'to',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'Popular',\n",
       " 'techniques',\n",
       " 'include',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'semantic',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'words',\n",
       " ',',\n",
       " 'and',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'end-to-end',\n",
       " 'learning',\n",
       " 'of',\n",
       " 'a',\n",
       " 'higher-level',\n",
       " 'task',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'question',\n",
       " 'answering',\n",
       " ')',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'relying',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pipeline',\n",
       " 'of',\n",
       " 'separate',\n",
       " 'intermediate',\n",
       " 'tasks',\n",
       " '(',\n",
       " 'e.g.',\n",
       " ',',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'dependency',\n",
       " 'parsing',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'areas',\n",
       " ',',\n",
       " 'this',\n",
       " 'shift',\n",
       " 'has',\n",
       " 'entailed',\n",
       " 'substantial',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'how',\n",
       " 'NLP',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'designed',\n",
       " ',',\n",
       " 'such',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-based',\n",
       " 'approaches',\n",
       " 'may',\n",
       " 'be',\n",
       " 'viewed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'paradigm',\n",
       " 'distinct',\n",
       " 'from',\n",
       " 'statistical',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'For',\n",
       " 'instance',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_wt = nltk.word_tokenize\n",
    "wiki_words = default_wt(wiki)\n",
    "wiki_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Some of the main features of this tokenizer** </font>\n",
    "* <font size=\"4\"> Splits and separates out periods that appear at the end of a sentence </font>\n",
    "* <font size=\"4\"> Splits and separates commas and single quotes when followed by whitespace </font>\n",
    "* <font size=\"4\"> Most punctuation characters are split and separated into independent tokens </font>\n",
    "* <font size=\"4\"> Splits words with standard contractions, such as don’t to do and n’t </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Besides, we could also use regular expressions to perform word tokenization.** <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'The',\n",
       " 'result',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'generation',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'has',\n",
       " 'its',\n",
       " 'roots',\n",
       " 'in',\n",
       " 'the',\n",
       " '1950s',\n",
       " 'Already',\n",
       " 'in',\n",
       " '1950',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'published',\n",
       " 'an',\n",
       " 'article',\n",
       " 'titled',\n",
       " 'Computing',\n",
       " 'Machinery',\n",
       " 'and',\n",
       " 'Intelligence',\n",
       " 'which',\n",
       " 'proposed',\n",
       " 'what',\n",
       " 'is',\n",
       " 'now',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Turing',\n",
       " 'test',\n",
       " 'as',\n",
       " 'a',\n",
       " 'criterion',\n",
       " 'of',\n",
       " 'intelligence',\n",
       " 'a',\n",
       " 'task',\n",
       " 'that',\n",
       " 'involves',\n",
       " 'the',\n",
       " 'automated',\n",
       " 'interpretation',\n",
       " 'and',\n",
       " 'generation',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'not',\n",
       " 'articulated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'The',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'symbolic',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'well',\n",
       " 'summarized',\n",
       " 'by',\n",
       " 'John',\n",
       " 'Searle',\n",
       " 's',\n",
       " 'Chinese',\n",
       " 'room',\n",
       " 'experiment',\n",
       " 'Given',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'rules',\n",
       " 'e',\n",
       " 'g',\n",
       " 'a',\n",
       " 'Chinese',\n",
       " 'phrasebook',\n",
       " 'with',\n",
       " 'questions',\n",
       " 'and',\n",
       " 'matching',\n",
       " 'answers',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'or',\n",
       " 'other',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'by',\n",
       " 'applying',\n",
       " 'those',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'the',\n",
       " 'data',\n",
       " 'it',\n",
       " 'is',\n",
       " 'confronted',\n",
       " 'with',\n",
       " 'Up',\n",
       " 'to',\n",
       " 'the',\n",
       " '1980s',\n",
       " 'most',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'based',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'sets',\n",
       " 'of',\n",
       " 'hand',\n",
       " 'written',\n",
       " 'rules',\n",
       " 'Starting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'however',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'for',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'This',\n",
       " 'was',\n",
       " 'due',\n",
       " 'to',\n",
       " 'both',\n",
       " 'the',\n",
       " 'steady',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'power',\n",
       " 'see',\n",
       " 'Moore',\n",
       " 's',\n",
       " 'law',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gradual',\n",
       " 'lessening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dominance',\n",
       " 'of',\n",
       " 'Chomskyan',\n",
       " 'theories',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " 'e',\n",
       " 'g',\n",
       " 'transformational',\n",
       " 'grammar',\n",
       " 'whose',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'discouraged',\n",
       " 'the',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'that',\n",
       " 'underlies',\n",
       " 'the',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'language',\n",
       " 'processing',\n",
       " '6',\n",
       " 'In',\n",
       " 'the',\n",
       " '2010s',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'style',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'methods',\n",
       " 'became',\n",
       " 'widespread',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'due',\n",
       " 'in',\n",
       " 'part',\n",
       " 'to',\n",
       " 'a',\n",
       " 'flurry',\n",
       " 'of',\n",
       " 'results',\n",
       " 'showing',\n",
       " 'that',\n",
       " 'such',\n",
       " 'techniques',\n",
       " '7',\n",
       " '8',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'results',\n",
       " 'in',\n",
       " 'many',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'tasks',\n",
       " 'for',\n",
       " 'example',\n",
       " 'in',\n",
       " 'language',\n",
       " 'modeling',\n",
       " '9',\n",
       " 'parsing',\n",
       " '10',\n",
       " '11',\n",
       " 'and',\n",
       " 'many',\n",
       " 'others',\n",
       " 'This',\n",
       " 'is',\n",
       " 'increasingly',\n",
       " 'important',\n",
       " 'in',\n",
       " 'medicine',\n",
       " 'and',\n",
       " 'healthcare',\n",
       " 'where',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'being',\n",
       " 'used',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'notes',\n",
       " 'and',\n",
       " 'text',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'health',\n",
       " 'records',\n",
       " 'that',\n",
       " 'would',\n",
       " 'otherwise',\n",
       " 'be',\n",
       " 'inaccessible',\n",
       " 'for',\n",
       " 'study',\n",
       " 'when',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'care',\n",
       " '12',\n",
       " 'In',\n",
       " 'the',\n",
       " 'early',\n",
       " 'days',\n",
       " 'many',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'designed',\n",
       " 'by',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " 'i',\n",
       " 'e',\n",
       " 'the',\n",
       " 'hand',\n",
       " 'coding',\n",
       " 'of',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'rules',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'lookup',\n",
       " '13',\n",
       " '14',\n",
       " 'such',\n",
       " 'as',\n",
       " 'by',\n",
       " 'writing',\n",
       " 'grammars',\n",
       " 'or',\n",
       " 'devising',\n",
       " 'heuristic',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'stemming',\n",
       " 'More',\n",
       " 'recent',\n",
       " 'systems',\n",
       " 'based',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'many',\n",
       " 'advantages',\n",
       " 'over',\n",
       " 'hand',\n",
       " 'produced',\n",
       " 'rules',\n",
       " 'Despite',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " 'are',\n",
       " 'still',\n",
       " '2020',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'so',\n",
       " 'called',\n",
       " 'statistical',\n",
       " 'revolution',\n",
       " '15',\n",
       " '16',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'and',\n",
       " 'mid',\n",
       " '1990s',\n",
       " 'much',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'research',\n",
       " 'has',\n",
       " 'relied',\n",
       " 'heavily',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'using',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rules',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'the',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'of',\n",
       " 'corpus',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents',\n",
       " 'possibly',\n",
       " 'with',\n",
       " 'human',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'annotations',\n",
       " 'of',\n",
       " 'typical',\n",
       " 'real',\n",
       " 'world',\n",
       " 'examples',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'classes',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " 'These',\n",
       " 'algorithms',\n",
       " 'take',\n",
       " 'as',\n",
       " 'input',\n",
       " 'a',\n",
       " 'large',\n",
       " 'set',\n",
       " 'of',\n",
       " 'features',\n",
       " 'that',\n",
       " 'are',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " 'Increasingly',\n",
       " 'however',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real',\n",
       " 'valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'each',\n",
       " 'input',\n",
       " 'feature',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'have',\n",
       " 'the',\n",
       " 'advantage',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'express',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'certainty',\n",
       " 'of',\n",
       " 'many',\n",
       " 'different',\n",
       " 'possible',\n",
       " 'answers',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'only',\n",
       " 'one',\n",
       " 'producing',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'such',\n",
       " 'a',\n",
       " 'model',\n",
       " 'is',\n",
       " 'included',\n",
       " 'as',\n",
       " 'a',\n",
       " 'component',\n",
       " 'of',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earliest',\n",
       " 'used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'such',\n",
       " 'as',\n",
       " 'decision',\n",
       " 'trees',\n",
       " 'produced',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'hard',\n",
       " 'if',\n",
       " 'then',\n",
       " 'rules',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'existing',\n",
       " 'hand',\n",
       " 'written',\n",
       " 'rules',\n",
       " 'However',\n",
       " 'part',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'tagging',\n",
       " 'introduced',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'hidden',\n",
       " 'Markov',\n",
       " 'models',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'increasingly',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real',\n",
       " 'valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'the',\n",
       " 'features',\n",
       " 'making',\n",
       " 'up',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data',\n",
       " 'The',\n",
       " 'cache',\n",
       " 'language',\n",
       " 'models',\n",
       " 'upon',\n",
       " 'which',\n",
       " 'many',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'systems',\n",
       " 'now',\n",
       " 'rely',\n",
       " 'are',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'such',\n",
       " 'statistical',\n",
       " 'models',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'more',\n",
       " 'robust',\n",
       " 'when',\n",
       " 'given',\n",
       " 'unfamiliar',\n",
       " 'input',\n",
       " 'especially',\n",
       " 'input',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'errors',\n",
       " 'as',\n",
       " 'is',\n",
       " 'very',\n",
       " 'common',\n",
       " 'for',\n",
       " 'real',\n",
       " 'world',\n",
       " 'data',\n",
       " 'and',\n",
       " 'produce',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'integrated',\n",
       " 'into',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " 'comprising',\n",
       " 'multiple',\n",
       " 'subtasks',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'neural',\n",
       " 'turn',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research',\n",
       " 'have',\n",
       " 'been',\n",
       " 'largely',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'However',\n",
       " 'they',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'be',\n",
       " 'relevant',\n",
       " 'for',\n",
       " 'contexts',\n",
       " 'in',\n",
       " 'which',\n",
       " 'statistical',\n",
       " 'interpretability',\n",
       " 'and',\n",
       " 'transparency',\n",
       " 'is',\n",
       " 'required',\n",
       " 'A',\n",
       " 'major',\n",
       " 'drawback',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'require',\n",
       " 'elaborate',\n",
       " 'feature',\n",
       " 'engineering',\n",
       " 'Since',\n",
       " '2015',\n",
       " '17',\n",
       " 'the',\n",
       " 'field',\n",
       " 'has',\n",
       " 'thus',\n",
       " 'largely',\n",
       " 'abandoned',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'and',\n",
       " 'shifted',\n",
       " 'to',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'Popular',\n",
       " 'techniques',\n",
       " 'include',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'semantic',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'words',\n",
       " 'and',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'end',\n",
       " 'to',\n",
       " 'end',\n",
       " 'learning',\n",
       " 'of',\n",
       " 'a',\n",
       " 'higher',\n",
       " 'level',\n",
       " 'task',\n",
       " 'e',\n",
       " 'g',\n",
       " 'question',\n",
       " 'answering',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'relying',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pipeline',\n",
       " 'of',\n",
       " 'separate',\n",
       " 'intermediate',\n",
       " 'tasks',\n",
       " 'e',\n",
       " 'g',\n",
       " 'part',\n",
       " 'of',\n",
       " 'speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'dependency',\n",
       " 'parsing',\n",
       " 'In',\n",
       " 'some',\n",
       " 'areas',\n",
       " 'this',\n",
       " 'shift',\n",
       " 'has',\n",
       " 'entailed',\n",
       " 'substantial',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'how',\n",
       " 'NLP',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'such',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'based',\n",
       " 'approaches',\n",
       " 'may',\n",
       " 'be',\n",
       " 'viewed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'paradigm',\n",
       " 'distinct',\n",
       " 'from',\n",
       " 'statistical',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'For',\n",
       " 'instance',\n",
       " 'the',\n",
       " 'term',\n",
       " 'neural',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'NMT',\n",
       " 'emphasizes',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'based',\n",
       " 'approaches',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'directly',\n",
       " 'learn',\n",
       " 'sequence',\n",
       " 'to',\n",
       " 'sequence',\n",
       " 'transformations',\n",
       " 'obviating',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'intermediate',\n",
       " 'steps',\n",
       " 'such',\n",
       " 'as',\n",
       " 'word',\n",
       " 'alignment',\n",
       " 'and',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'that',\n",
       " 'was',\n",
       " 'used',\n",
       " 'in',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'SMT',\n",
       " 'Latest',\n",
       " 'works',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'use',\n",
       " 'non',\n",
       " 'technical',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'a',\n",
       " 'given',\n",
       " 'task',\n",
       " 'to',\n",
       " 'build',\n",
       " 'proper',\n",
       " 'neural',\n",
       " 'network',\n",
       " '18',\n",
       " 'The',\n",
       " 'following',\n",
       " 'is',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'commonly',\n",
       " 'researched',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'these',\n",
       " 'tasks',\n",
       " 'have',\n",
       " 'direct',\n",
       " 'real',\n",
       " 'world',\n",
       " 'applications',\n",
       " 'while',\n",
       " 'others',\n",
       " 'more',\n",
       " 'commonly',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'subtasks',\n",
       " 'that',\n",
       " 'are',\n",
       " 'used',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the pattern to identify tokens themselves\n",
    "TOKEN_PATTERN = r'\\w+' # alphanumerics \n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "wiki_words = regex_wt.tokenize(wiki)\n",
    "wiki_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(NLP)',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics,',\n",
       " 'computer',\n",
       " 'science,',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language,',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data.',\n",
       " 'The',\n",
       " 'result',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " '\"understanding\"',\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents,',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves.',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition,',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding,',\n",
       " 'and',\n",
       " 'natural-language',\n",
       " 'generation.',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'has',\n",
       " 'its',\n",
       " 'roots',\n",
       " 'in',\n",
       " 'the',\n",
       " '1950s.',\n",
       " 'Already',\n",
       " 'in',\n",
       " '1950,',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'published',\n",
       " 'an',\n",
       " 'article',\n",
       " 'titled',\n",
       " '\"Computing',\n",
       " 'Machinery',\n",
       " 'and',\n",
       " 'Intelligence\"',\n",
       " 'which',\n",
       " 'proposed',\n",
       " 'what',\n",
       " 'is',\n",
       " 'now',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Turing',\n",
       " 'test',\n",
       " 'as',\n",
       " 'a',\n",
       " 'criterion',\n",
       " 'of',\n",
       " 'intelligence,',\n",
       " 'a',\n",
       " 'task',\n",
       " 'that',\n",
       " 'involves',\n",
       " 'the',\n",
       " 'automated',\n",
       " 'interpretation',\n",
       " 'and',\n",
       " 'generation',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language,',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'not',\n",
       " 'articulated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'artificial',\n",
       " 'intelligence.',\n",
       " 'The',\n",
       " 'premise',\n",
       " 'of',\n",
       " 'symbolic',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'well-summarized',\n",
       " 'by',\n",
       " 'John',\n",
       " \"Searle's\",\n",
       " 'Chinese',\n",
       " 'room',\n",
       " 'experiment:',\n",
       " 'Given',\n",
       " 'a',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'rules',\n",
       " '(e.g.,',\n",
       " 'a',\n",
       " 'Chinese',\n",
       " 'phrasebook,',\n",
       " 'with',\n",
       " 'questions',\n",
       " 'and',\n",
       " 'matching',\n",
       " 'answers),',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'emulates',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '(or',\n",
       " 'other',\n",
       " 'NLP',\n",
       " 'tasks)',\n",
       " 'by',\n",
       " 'applying',\n",
       " 'those',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'the',\n",
       " 'data',\n",
       " 'it',\n",
       " 'is',\n",
       " 'confronted',\n",
       " 'with.',\n",
       " 'Up',\n",
       " 'to',\n",
       " 'the',\n",
       " '1980s,',\n",
       " 'most',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'based',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'sets',\n",
       " 'of',\n",
       " 'hand-written',\n",
       " 'rules.',\n",
       " 'Starting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s,',\n",
       " 'however,',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'revolution',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'the',\n",
       " 'introduction',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'for',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'This',\n",
       " 'was',\n",
       " 'due',\n",
       " 'to',\n",
       " 'both',\n",
       " 'the',\n",
       " 'steady',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'computational',\n",
       " 'power',\n",
       " '(see',\n",
       " \"Moore's\",\n",
       " 'law)',\n",
       " 'and',\n",
       " 'the',\n",
       " 'gradual',\n",
       " 'lessening',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dominance',\n",
       " 'of',\n",
       " 'Chomskyan',\n",
       " 'theories',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " '(e.g.',\n",
       " 'transformational',\n",
       " 'grammar),',\n",
       " 'whose',\n",
       " 'theoretical',\n",
       " 'underpinnings',\n",
       " 'discouraged',\n",
       " 'the',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'that',\n",
       " 'underlies',\n",
       " 'the',\n",
       " 'machine-learning',\n",
       " 'approach',\n",
       " 'to',\n",
       " 'language',\n",
       " 'processing.[6]',\n",
       " 'In',\n",
       " 'the',\n",
       " '2010s,',\n",
       " 'representation',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-style',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'methods',\n",
       " 'became',\n",
       " 'widespread',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing,',\n",
       " 'due',\n",
       " 'in',\n",
       " 'part',\n",
       " 'to',\n",
       " 'a',\n",
       " 'flurry',\n",
       " 'of',\n",
       " 'results',\n",
       " 'showing',\n",
       " 'that',\n",
       " 'such',\n",
       " 'techniques[7][8]',\n",
       " 'can',\n",
       " 'achieve',\n",
       " 'state-of-the-art',\n",
       " 'results',\n",
       " 'in',\n",
       " 'many',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'tasks,',\n",
       " 'for',\n",
       " 'example',\n",
       " 'in',\n",
       " 'language',\n",
       " 'modeling,[9]',\n",
       " 'parsing,[10][11]',\n",
       " 'and',\n",
       " 'many',\n",
       " 'others.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'increasingly',\n",
       " 'important',\n",
       " 'in',\n",
       " 'medicine',\n",
       " 'and',\n",
       " 'healthcare,',\n",
       " 'where',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'being',\n",
       " 'used',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'notes',\n",
       " 'and',\n",
       " 'text',\n",
       " 'in',\n",
       " 'electronic',\n",
       " 'health',\n",
       " 'records',\n",
       " 'that',\n",
       " 'would',\n",
       " 'otherwise',\n",
       " 'be',\n",
       " 'inaccessible',\n",
       " 'for',\n",
       " 'study',\n",
       " 'when',\n",
       " 'seeking',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'care.[12]',\n",
       " 'In',\n",
       " 'the',\n",
       " 'early',\n",
       " 'days,',\n",
       " 'many',\n",
       " 'language-processing',\n",
       " 'systems',\n",
       " 'were',\n",
       " 'designed',\n",
       " 'by',\n",
       " 'symbolic',\n",
       " 'methods,',\n",
       " 'i.e.,',\n",
       " 'the',\n",
       " 'hand-coding',\n",
       " 'of',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'rules,',\n",
       " 'coupled',\n",
       " 'with',\n",
       " 'a',\n",
       " 'dictionary',\n",
       " 'lookup:[13][14]',\n",
       " 'such',\n",
       " 'as',\n",
       " 'by',\n",
       " 'writing',\n",
       " 'grammars',\n",
       " 'or',\n",
       " 'devising',\n",
       " 'heuristic',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'stemming.',\n",
       " 'More',\n",
       " 'recent',\n",
       " 'systems',\n",
       " 'based',\n",
       " 'on',\n",
       " 'machine-learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'many',\n",
       " 'advantages',\n",
       " 'over',\n",
       " 'hand-produced',\n",
       " 'rules:',\n",
       " 'Despite',\n",
       " 'the',\n",
       " 'popularity',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research,',\n",
       " 'symbolic',\n",
       " 'methods',\n",
       " 'are',\n",
       " 'still',\n",
       " '(2020)',\n",
       " 'commonly',\n",
       " 'used',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'so-called',\n",
       " '\"statistical',\n",
       " 'revolution\"[15][16]',\n",
       " 'in',\n",
       " 'the',\n",
       " 'late',\n",
       " '1980s',\n",
       " 'and',\n",
       " 'mid-1990s,',\n",
       " 'much',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'research',\n",
       " 'has',\n",
       " 'relied',\n",
       " 'heavily',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'using',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rules',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'large',\n",
       " 'corpora',\n",
       " '(the',\n",
       " 'plural',\n",
       " 'form',\n",
       " 'of',\n",
       " 'corpus,',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents,',\n",
       " 'possibly',\n",
       " 'with',\n",
       " 'human',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'annotations)',\n",
       " 'of',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'examples.',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'classes',\n",
       " 'of',\n",
       " 'machine-learning',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'been',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'natural-language-processing',\n",
       " 'tasks.',\n",
       " 'These',\n",
       " 'algorithms',\n",
       " 'take',\n",
       " 'as',\n",
       " 'input',\n",
       " 'a',\n",
       " 'large',\n",
       " 'set',\n",
       " 'of',\n",
       " '\"features\"',\n",
       " 'that',\n",
       " 'are',\n",
       " 'generated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data.',\n",
       " 'Increasingly,',\n",
       " 'however,',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models,',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft,',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'each',\n",
       " 'input',\n",
       " 'feature.',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'have',\n",
       " 'the',\n",
       " 'advantage',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'express',\n",
       " 'the',\n",
       " 'relative',\n",
       " 'certainty',\n",
       " 'of',\n",
       " 'many',\n",
       " 'different',\n",
       " 'possible',\n",
       " 'answers',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'only',\n",
       " 'one,',\n",
       " 'producing',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'such',\n",
       " 'a',\n",
       " 'model',\n",
       " 'is',\n",
       " 'included',\n",
       " 'as',\n",
       " 'a',\n",
       " 'component',\n",
       " 'of',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'earliest-used',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'algorithms,',\n",
       " 'such',\n",
       " 'as',\n",
       " 'decision',\n",
       " 'trees,',\n",
       " 'produced',\n",
       " 'systems',\n",
       " 'of',\n",
       " 'hard',\n",
       " 'if-then',\n",
       " 'rules',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'existing',\n",
       " 'hand-written',\n",
       " 'rules.',\n",
       " 'However,',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'introduced',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'hidden',\n",
       " 'Markov',\n",
       " 'models',\n",
       " 'to',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'increasingly,',\n",
       " 'research',\n",
       " 'has',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'statistical',\n",
       " 'models,',\n",
       " 'which',\n",
       " 'make',\n",
       " 'soft,',\n",
       " 'probabilistic',\n",
       " 'decisions',\n",
       " 'based',\n",
       " 'on',\n",
       " 'attaching',\n",
       " 'real-valued',\n",
       " 'weights',\n",
       " 'to',\n",
       " 'the',\n",
       " 'features',\n",
       " 'making',\n",
       " 'up',\n",
       " 'the',\n",
       " 'input',\n",
       " 'data.',\n",
       " 'The',\n",
       " 'cache',\n",
       " 'language',\n",
       " 'models',\n",
       " 'upon',\n",
       " 'which',\n",
       " 'many',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'systems',\n",
       " 'now',\n",
       " 'rely',\n",
       " 'are',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'such',\n",
       " 'statistical',\n",
       " 'models.',\n",
       " 'Such',\n",
       " 'models',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'more',\n",
       " 'robust',\n",
       " 'when',\n",
       " 'given',\n",
       " 'unfamiliar',\n",
       " 'input,',\n",
       " 'especially',\n",
       " 'input',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'errors',\n",
       " '(as',\n",
       " 'is',\n",
       " 'very',\n",
       " 'common',\n",
       " 'for',\n",
       " 'real-world',\n",
       " 'data),',\n",
       " 'and',\n",
       " 'produce',\n",
       " 'more',\n",
       " 'reliable',\n",
       " 'results',\n",
       " 'when',\n",
       " 'integrated',\n",
       " 'into',\n",
       " 'a',\n",
       " 'larger',\n",
       " 'system',\n",
       " 'comprising',\n",
       " 'multiple',\n",
       " 'subtasks.',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'neural',\n",
       " 'turn,',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'research',\n",
       " 'have',\n",
       " 'been',\n",
       " 'largely',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'neural',\n",
       " 'networks.',\n",
       " 'However,',\n",
       " 'they',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'be',\n",
       " 'relevant',\n",
       " 'for',\n",
       " 'contexts',\n",
       " 'in',\n",
       " 'which',\n",
       " 'statistical',\n",
       " 'interpretability',\n",
       " 'and',\n",
       " 'transparency',\n",
       " 'is',\n",
       " 'required.',\n",
       " 'A',\n",
       " 'major',\n",
       " 'drawback',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'require',\n",
       " 'elaborate',\n",
       " 'feature',\n",
       " 'engineering.',\n",
       " 'Since',\n",
       " '2015,[17]',\n",
       " 'the',\n",
       " 'field',\n",
       " 'has',\n",
       " 'thus',\n",
       " 'largely',\n",
       " 'abandoned',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'and',\n",
       " 'shifted',\n",
       " 'to',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning.',\n",
       " 'Popular',\n",
       " 'techniques',\n",
       " 'include',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'word',\n",
       " 'embeddings',\n",
       " 'to',\n",
       " 'capture',\n",
       " 'semantic',\n",
       " 'properties',\n",
       " 'of',\n",
       " 'words,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'end-to-end',\n",
       " 'learning',\n",
       " 'of',\n",
       " 'a',\n",
       " 'higher-level',\n",
       " 'task',\n",
       " '(e.g.,',\n",
       " 'question',\n",
       " 'answering)',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'relying',\n",
       " 'on',\n",
       " 'a',\n",
       " 'pipeline',\n",
       " 'of',\n",
       " 'separate',\n",
       " 'intermediate',\n",
       " 'tasks',\n",
       " '(e.g.,',\n",
       " 'part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'dependency',\n",
       " 'parsing).',\n",
       " 'In',\n",
       " 'some',\n",
       " 'areas,',\n",
       " 'this',\n",
       " 'shift',\n",
       " 'has',\n",
       " 'entailed',\n",
       " 'substantial',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'how',\n",
       " 'NLP',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'designed,',\n",
       " 'such',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network-based',\n",
       " 'approaches',\n",
       " 'may',\n",
       " 'be',\n",
       " 'viewed',\n",
       " 'as',\n",
       " 'a',\n",
       " 'new',\n",
       " 'paradigm',\n",
       " 'distinct',\n",
       " 'from',\n",
       " 'statistical',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'For',\n",
       " 'instance,',\n",
       " 'the',\n",
       " 'term',\n",
       " 'neural',\n",
       " 'machine',\n",
       " 'translation',\n",
       " '(NMT)',\n",
       " 'emphasizes',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'deep',\n",
       " 'learning-based',\n",
       " 'approaches',\n",
       " 'to',\n",
       " 'machine',\n",
       " 'translation',\n",
       " 'directly',\n",
       " 'learn',\n",
       " 'sequence-to-sequence',\n",
       " 'transformations,',\n",
       " 'obviating',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'intermediate',\n",
       " 'steps',\n",
       " 'such',\n",
       " 'as',\n",
       " 'word',\n",
       " 'alignment',\n",
       " 'and',\n",
       " 'language',\n",
       " 'modeling',\n",
       " 'that',\n",
       " 'was',\n",
       " 'used',\n",
       " 'in',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'translation',\n",
       " '(SMT).',\n",
       " 'Latest',\n",
       " 'works',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'use',\n",
       " 'non-technical',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'a',\n",
       " 'given',\n",
       " 'task',\n",
       " 'to',\n",
       " 'build',\n",
       " 'proper',\n",
       " 'neural',\n",
       " 'network.[18]',\n",
       " 'The',\n",
       " 'following',\n",
       " 'is',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'commonly',\n",
       " 'researched',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'these',\n",
       " 'tasks',\n",
       " 'have',\n",
       " 'direct',\n",
       " 'real-world',\n",
       " 'applications,',\n",
       " 'while',\n",
       " 'others',\n",
       " 'more',\n",
       " 'commonly',\n",
       " 'serve',\n",
       " 'as',\n",
       " 'subtasks',\n",
       " 'that',\n",
       " 'are',\n",
       " 'used',\n",
       " 'to',\n",
       " 'aid',\n",
       " 'in',\n",
       " 'solving',\n",
       " 'larger',\n",
       " 'tasks.',\n",
       " 'Though',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " 'are',\n",
       " 'closely',\n",
       " 'intertwined,',\n",
       " 'they',\n",
       " 'can',\n",
       " 'be',\n",
       " 'subdivided',\n",
       " 'into',\n",
       " 'categories',\n",
       " 'for',\n",
       " 'convenience.',\n",
       " 'A',\n",
       " 'coarse',\n",
       " 'division',\n",
       " 'is',\n",
       " 'given',\n",
       " 'below.',\n",
       " 'Based',\n",
       " 'on',\n",
       " 'long-standing',\n",
       " 'trends',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'extrapolate',\n",
       " 'future',\n",
       " 'directions',\n",
       " 'of',\n",
       " 'NLP.',\n",
       " 'As',\n",
       " 'of',\n",
       " '2020,',\n",
       " 'three',\n",
       " 'trends',\n",
       " 'among',\n",
       " 'the',\n",
       " 'topics',\n",
       " 'of',\n",
       " 'the',\n",
       " 'long-standing',\n",
       " 'series',\n",
       " 'of',\n",
       " 'CoNLL',\n",
       " 'Shared',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern to identify tokens by using gaps between tokens\n",
    "GAP_PATTERN = r'\\s+' # whitespace\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "wiki_words = regex_wt.tokenize(wiki)\n",
    "wiki_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **3. Stemming**</font>  <br>\n",
    "<font size=\"4\"> $\\textbf{Problem}$: Distinguishing the words like \"replace\", \"replaced\", \"replacement\", \"replaces\", and \"replacing\" could lead to overfitting. <br>\n",
    "<span style=\"color:blue\">$\\textbf{Solution}$</span>: Representing each word using its word stem, which\n",
    "involves identifying (or conflating) all the words that have the same word stem. If this is relealized using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('jump', 'jump', 'jump', 'lie')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "P_Stemmer = PorterStemmer()\n",
    "P_Stemmer.stem('jumping'), P_Stemmer.stem('jumps'), P_Stemmer.stem('jumped'), P_Stemmer.stem('lying')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"Our meeting today was worse than yesterday, I'm scared of meeting the clients tomorrow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'meet',\n",
       " 'today',\n",
       " 'wa',\n",
       " 'wors',\n",
       " 'than',\n",
       " 'yesterday,',\n",
       " \"i'm\",\n",
       " 'scare',\n",
       " 'of',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'client',\n",
       " 'tomorrow.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[P_Stemmer.stem(token) for token in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **4. Lemmatization**</font>  <br>\n",
    "<font size=\"4\"> When we replace the word with the word stem, if a dictionary of known word forms is used (an\n",
    "explicit and human-verified system), and the role of the word in the sentence is taken into account, the process is referred to as lemmatization. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "eat\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# lemmatize nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "# lemmatize verbs\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "# lemmatize adjectives\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print( wnl.lemmatize('ate', 'n') )\n",
    "print( wnl.lemmatize('fancier', 'v') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # speech tagging and effective lemmatization for each token in a text document\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'meeting',\n",
       " 'today',\n",
       " 'be',\n",
       " 'bad',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " ',',\n",
       " 'I',\n",
       " 'be',\n",
       " 'scared',\n",
       " 'of',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'client',\n",
       " 'tomorrow',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in en_nlp(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **5. Removing stopwords**</font>  <br>\n",
    "<font size=\"4\"> Usually there are a lot of uninformative words in the text. They are useless and consume the calculating power at the same time. To get rid of them, you could either \n",
    "* select words by using a language-specific list of stopwords, or\n",
    "* discard words that appear too frequently\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "First 20th stopword:\n",
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "print(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"First 20th stopword:\\n{}\".format(sorted(list(ENGLISH_STOP_WORDS))[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Discarding words based on thier frequencies is embedded in the feature engineering package, e.g., setting the *max_df* option in *CountVectorizer*.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Other text cleaning procedures, such as removing accented characters, removing special characters can be realized with the help of the powerful regular expression.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:red\"> <font size=\"7\"> Part 3 Feature Engineering </font>  </span> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Machine learning algorithms cannot deal with the characters easily. <br>\n",
    " We have to set up the representation of the documents.** \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **1. Bag of words model** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> \n",
    "In this representation, we discard most of the structure of the input text, like chapters, paragraphs, sentences, and formatting, and only count how often each word appears in each text in the corpus. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "<span style=\"color:blue\">$\\textbf{3 Steps}$</span> : Tokenization $\\rightarrow$ Vocabulary building $\\rightarrow$ Encoding </font> \n",
    "<img src=\"file/bag_words.png\", style=\"width: 650px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "test_words =[\"The fool doth think he is wise,\", \n",
    "              \"but the wise man knows himself to be a fool\"]  # the toy dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(test_words)  # build our vocabulary\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_))) # access vocabulary via vocabulary_ attribute\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(test_words)  # create the bag-of-words representation for the training data\n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words))) \n",
    "# the representation is stored in a SciPy sparse matrix that only stores nonzero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **2. Bag of N-Grams model** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> \n",
    "    $\\dagger$ One main drawback of the bag of words model: the  word order is completely discarded. <br>\n",
    "</font>\n",
    "<font size=\"4\"> \n",
    "    <center> “it’s bad, not good at all” = “it’s good, not bad at all” in this representation <center>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "<span style=\"color:blue\">$\\textbf{Solution}$</span>: Taking pairs or triplets of tokens that appear next to each other into account also. In this way, we extend the bag of words model to the bag of N-Grams model. N-Grams: the maximum length of the sequences of tokens that is considered.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The fool doth think he is wise,',\n",
       " 'but the wise man knows himself to be a fool']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 27\n",
      "Vocabulary:\n",
      "['be', 'be fool', 'but', 'but the', 'doth', 'doth think', 'fool', 'fool doth', 'he', 'he is', 'himself', 'himself to', 'is', 'is wise', 'knows', 'knows himself', 'man', 'man knows', 'the', 'the fool', 'the wise', 'think', 'think he', 'to', 'to be', 'wise', 'wise man']\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2)).fit(test_words)\n",
    "print(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\n",
    "print(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>be</th>\n",
       "      <th>be fool</th>\n",
       "      <th>but</th>\n",
       "      <th>but the</th>\n",
       "      <th>doth</th>\n",
       "      <th>doth think</th>\n",
       "      <th>fool</th>\n",
       "      <th>fool doth</th>\n",
       "      <th>he</th>\n",
       "      <th>he is</th>\n",
       "      <th>...</th>\n",
       "      <th>man knows</th>\n",
       "      <th>the</th>\n",
       "      <th>the fool</th>\n",
       "      <th>the wise</th>\n",
       "      <th>think</th>\n",
       "      <th>think he</th>\n",
       "      <th>to</th>\n",
       "      <th>to be</th>\n",
       "      <th>wise</th>\n",
       "      <th>wise man</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   be  be fool  but  but the  doth  doth think  fool  fool doth  he  he is  \\\n",
       "0   0        0    0        0     1           1     1          1   1      1   \n",
       "1   1        1    1        1     0           0     1          0   0      0   \n",
       "\n",
       "   ...  man knows  the  the fool  the wise  think  think he  to  to be  wise  \\\n",
       "0  ...          0    1         1         0      1         1   0      0     1   \n",
       "1  ...          1    1         0         1      0         0   1      1     1   \n",
       "\n",
       "   wise man  \n",
       "0         0  \n",
       "1         1  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_matrix = CountVectorizer(ngram_range=(1, 2)).fit_transform(test_words)\n",
    "cv_matrix = cv_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(cv_matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **3. TF-IDF model** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> * *TF-IDF: term frequency–inverse document frequency*  <br>\n",
    "<span style=\"color:blue\">$\\textbf{Basic idea}$</span>:\n",
    "If a word appears often in a particular document, but not in very many documents, it is likely\n",
    "to be very descriptive of the content of that document. We give high weight to any term that appears\n",
    "often in a particular document, but not in many documents in the corpus.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "    The tf-idf score for word w in document d can be calculated via\n",
    "</font>\n",
    "<img src=\"file/tf_idf_score.png\", style=\"width: 450px;\"/>,\n",
    "<font size=\"3\">\n",
    "where N is the number of documents in the training set, $N_{\\text{W}}$ is the number of documents in the training set that the word w appears in, and tf (the term frequency) is the number of times that the word w appears in the query document d. The representation of each document is rescaled to have Euclidean norm 1 (L2 normalization). In this way, the length of a document does not change the vectorized representation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sky is blue and beautiful.</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this blue and beautiful sky!</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A king's breakfast has sausages, ham, bacon, e...</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love green eggs, ham, sausages and bacon!</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The brown fox is quick and the blue dog is lazy!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The sky is very blue and the sky is very beaut...</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The dog is lazy but the brown fox is quick!</td>\n",
       "      <td>animals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document Category\n",
       "0                     The sky is blue and beautiful.  weather\n",
       "1                  Love this blue and beautiful sky!  weather\n",
       "2       The quick brown fox jumps over the lazy dog.  animals\n",
       "3  A king's breakfast has sausages, ham, bacon, e...     food\n",
       "4        I love green eggs, ham, sausages and bacon!     food\n",
       "5   The brown fox is quick and the blue dog is lazy!  animals\n",
       "6  The sky is very blue and the sky is very beaut...  weather\n",
       "7        The dog is lazy but the brown fox is quick!  animals"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]\n",
    "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
    "\n",
    "corpus = np.array(corpus)\n",
    "corpus_df = pd.DataFrame({'Document': corpus, \n",
    "                          'Category': labels})\n",
    "corpus_df = corpus_df[['Document', 'Category']]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(min_df=0., max_df=1.)\n",
    "cv_matrix = cv.fit_transform(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bacon</th>\n",
       "      <th>beans</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>blue</th>\n",
       "      <th>breakfast</th>\n",
       "      <th>brown</th>\n",
       "      <th>dog</th>\n",
       "      <th>eggs</th>\n",
       "      <th>fox</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>jumps</th>\n",
       "      <th>kings</th>\n",
       "      <th>lazy</th>\n",
       "      <th>love</th>\n",
       "      <th>quick</th>\n",
       "      <th>sausages</th>\n",
       "      <th>sky</th>\n",
       "      <th>toast</th>\n",
       "      <th>today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n",
       "0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n",
       "3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n",
       "4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n",
       "5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n",
       "6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n",
       "7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n",
       "\n",
       "    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n",
       "0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n",
       "1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n",
       "2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n",
       "3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n",
       "4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n",
       "5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n",
       "6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n",
       "7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "tt_matrix = tt.fit_transform(cv_matrix)\n",
    "\n",
    "tt_matrix = tt_matrix.toarray()\n",
    "vocab = cv.get_feature_names()\n",
    "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:red\"> <font size=\"7\"> Part 4 Analysis with Machine Learning </font> </span> <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **1. Text classification** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Main task: categorizing or classifying text documents into various (predefined) categories based on inherent properties or attributes of each text document, e.g., email spam identification and news categorization.  </font> \n",
    "<img src=\"file/text_class.png\", style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Target Label  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
       "1  My brother is in the market for a high-perform...             3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
       "4  1)    I have an old Jasmine drive which I cann...             4   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...            12   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n",
       "9  \\nIf a Christian means someone who believes in...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups(subset='all', shuffle=True,\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))\n",
    "corpus, target_labels, target_names = (data.data, data.target, \n",
    "                                       [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.head(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_normalizer as tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize our corpus\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "norm_corpus = tn.normalize_corpus(corpus=data_df['Article'], html_stripping=True, contraction_expansion=True, \n",
    "                                  accented_char_removal=True, text_lower_case=True, text_lemmatization=True, \n",
    "                                  text_stemming=False, special_char_removal=True, remove_digits=True,\n",
    "                                  stopword_removal=True, stopwords=stopword_list)\n",
    "data_df['Clean Article'] = norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Clean Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>sure basher pens fan pretty confused lack kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>brother market high performance video card sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>finally say dream mediterranean new area great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>think scsi card dma transfer disk scsi card dm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>old jasmine drive use new system understanding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>\\nI just called them and they said the order w...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>call say order go th put ups tracer watch wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>\\input amstex\\n\\documentstyle{amsppt}\\n\\pagewi...</td>\n",
       "      <td>18</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>input amstex documentstyle amsppt pagewidth ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>This may be a simple question but:\\n\\nWe have ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>may simple question number pc use link mainfra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>I missed the first article[s] on this line due...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>miss first articles line due chance read news ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>] &gt; Try reading between the lines David - ther...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>try read line david strong hint angle nren nex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Article  Target Label  \\\n",
       "0    \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
       "1    My brother is in the market for a high-perform...             3   \n",
       "2    \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
       "3    \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
       "4    1)    I have an old Jasmine drive which I cann...             4   \n",
       "..                                                 ...           ...   \n",
       "795  \\nI just called them and they said the order w...             8   \n",
       "796  \\input amstex\\n\\documentstyle{amsppt}\\n\\pagewi...            18   \n",
       "797  This may be a simple question but:\\n\\nWe have ...             2   \n",
       "798  I missed the first article[s] on this line due...            12   \n",
       "799  ] > Try reading between the lines David - ther...            11   \n",
       "\n",
       "                  Target Name  \\\n",
       "0            rec.sport.hockey   \n",
       "1    comp.sys.ibm.pc.hardware   \n",
       "2       talk.politics.mideast   \n",
       "3    comp.sys.ibm.pc.hardware   \n",
       "4       comp.sys.mac.hardware   \n",
       "..                        ...   \n",
       "795           rec.motorcycles   \n",
       "796        talk.politics.misc   \n",
       "797   comp.os.ms-windows.misc   \n",
       "798           sci.electronics   \n",
       "799                 sci.crypt   \n",
       "\n",
       "                                         Clean Article  \n",
       "0    sure basher pens fan pretty confused lack kind...  \n",
       "1    brother market high performance video card sup...  \n",
       "2    finally say dream mediterranean new area great...  \n",
       "3    think scsi card dma transfer disk scsi card dm...  \n",
       "4    old jasmine drive use new system understanding...  \n",
       "..                                                 ...  \n",
       "795  call say order go th put ups tracer watch wait...  \n",
       "796  input amstex documentstyle amsppt pagewidth ma...  \n",
       "797  may simple question number pc use link mainfra...  \n",
       "798  miss first articles line due chance read news ...  \n",
       "799  try read line david strong hint angle nren nex...  \n",
       "\n",
       "[800 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((536,), (264,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                       np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Train Count</th>\n",
       "      <th>Test Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Target Label  Train Count  Test Count\n",
       "6            sci.electronics           41           7\n",
       "9     soc.religion.christian           34           8\n",
       "5             comp.windows.x           33          19\n",
       "15  comp.sys.ibm.pc.hardware           32          15\n",
       "7      comp.sys.mac.hardware           30          10\n",
       "0    comp.os.ms-windows.misc           29          14\n",
       "17                   sci.med           29          12\n",
       "14          rec.sport.hockey           28          17\n",
       "19           rec.motorcycles           28          14\n",
       "1              comp.graphics           27          18\n",
       "18                 sci.space           27          13\n",
       "16     talk.politics.mideast           27           9\n",
       "3                  sci.crypt           26          10\n",
       "12              misc.forsale           24          11\n",
       "4         talk.religion.misc           23          12\n",
       "11               alt.atheism           23          10\n",
       "8         talk.politics.misc           22           9\n",
       "2         talk.politics.guns           20          16\n",
       "10                 rec.autos           18          17\n",
       "13        rec.sport.baseball           15          23"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], \n",
    "             columns=['Target Label', 'Train Count', 'Test Count']).sort_values(by=['Train Count', 'Test Count'],\n",
    "             ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build BOW features on train articles\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (536, 9698)  Test features shape: (264, 9698)\n"
     ]
    }
   ],
   "source": [
    "print('Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **Naive Bayes classifiers** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.30555556 0.34579439 0.27102804 0.39252336 0.31775701]\n",
      "Mean CV Accuracy: 0.32653167185877463\n",
      "Test Accuracy: 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB  # suitable for classification with discrete features\n",
    "from sklearn.model_selection import cross_val_score # evaluate the performance by cross-validation method\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Data splitting in five-fold cross-validation.  </font> \n",
    "<img src=\"file/cross_val.png\", style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **LogisticRegression classifiers** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.37962963 0.3364486  0.31775701 0.3364486  0.28037383]\n",
      "Mean CV Accuracy: 0.33013153340256146\n",
      "Test Accuracy: 0.39015151515151514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **RandomForest classifiers** </font> <br>\n",
    "A collection of decision trees, where each tree is slightly different from the others. The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data. If we build many trees, all of which work well and overfit in different ways, we can reduce the amount\n",
    "of overfitting by averaging their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.2962963  0.21495327 0.21495327 0.1682243  0.22429907]\n",
      "Mean CV Accuracy: 0.22374524056767048\n",
      "Test Accuracy: 0.25757575757575757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **2. Sentiment analysis** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Task: analyzing some textual documents and predicting the sentiment or opinion based on the content of these documents. <br>\n",
    "<span style=\"color:red\">Basic idea</span>: to extract important information from the text and derive qualitative outputs like the overall sentiment being on a positive, neutral, or negative scale and quantitative outputs like the sentiment polarity,\n",
    "subjectivity, and objectivity proportions.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **Unsupervised lexicon-based models** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "<span style=\"color:red\">Basic idea</span>: A lexicon model typically uses a lexicon, also known as a dictionary or vocabulary of words specifically aligned to sentiment analysis. These lexicons contain a list of words associated with positive and negative sentiment, polarity (magnitude of negative or positive score), parts of speech (POS) tags, subjectivity classifiers (strong, weak, neutral), mood, modality, and so on.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_evaluation_utils as meu\n",
    "import textblob  # TextBlob Lexicon\n",
    "# The lexicon can be found at https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: <br>\n",
    "word form=\"able\" cornetto_synset_id=\"n_a-534450\" wordnet_id=\"a-01017439\"\n",
    "pos=\"JJ\" sense=\"having a strong healthy body\" polarity=\"0.5\"\n",
    "subjectivity=\"1.0\" intensity=\"1.0\" confidence=\"0.9\" <br>\n",
    "word form=\"abhorrent\" wordnet_id=\"a-1625063\" pos=\"JJ\" sense=\"offensive\n",
    "to the mind\" polarity=\"-0.7\" subjectivity=\"0.8\" intensity=\"1.0\"\n",
    "reliability=\"0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('movie_reviews.csv')\n",
    "\n",
    "reviews = np.array(dataset['review'])\n",
    "sentiments = np.array(dataset['sentiment'])\n",
    "\n",
    "# extract data for model evaluation\n",
    "test_reviews = reviews[35000:] # 15,000 for testing\n",
    "test_sentiments = sentiments[35000:]\n",
    "sample_review_ids = [7626, 3533, 13010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW: no comment - stupid movie, acting average or worse... screenplay - no sense at all... SKIP IT!\n",
      "Actual Sentiment: negative\n",
      "Predicted Sentiment polarity: -0.3625\n",
      "------------------------------------------------------------\n",
      "REVIEW: I don't care if some people voted this movie to be bad. If you want the Truth this is a Very Good Movie! It has every thing a movie should have. You really should Get this one.\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: 0.16666666666666674\n",
      "------------------------------------------------------------\n",
      "REVIEW: Worst horror film ever but funniest film ever rolled in one you have got to see this film it is so cheap it is unbeliaveble but you have to see it really!!!! P.s watch the carrot\n",
      "Actual Sentiment: positive\n",
      "Predicted Sentiment polarity: -0.037239583333333326\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n",
    "    print('REVIEW:', review)\n",
    "    print('Actual Sentiment:', sentiment)\n",
    "    print('Predicted Sentiment polarity:', textblob.TextBlob(review).sentiment.polarity)\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "Typically, a positive score denotes positive sentiment and a negative score denotes negative sentiment. You can use a specific custom threshold to determine what should be positive or negative.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.7669\n",
      "Precision: 0.767\n",
      "Recall: 0.7669\n",
      "F1 Score: 0.7668\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.76      0.78      0.77      7510\n",
      "    negative       0.77      0.76      0.76      7490\n",
      "\n",
      "    accuracy                           0.77     15000\n",
      "   macro avg       0.77      0.77      0.77     15000\n",
      "weighted avg       0.77      0.77      0.77     15000\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive       5835     1675\n",
      "        negative       1822     5668\n"
     ]
    }
   ],
   "source": [
    "sentiment_polarity = [textblob.TextBlob(review).sentiment.polarity for review in test_reviews]\n",
    "predicted_sentiments = ['positive' if score >= 0.1 else 'negative' for score in sentiment_polarity] # set threshold at 0.1\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n",
    "                                  classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **Supervised models** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "train_reviews = reviews[:1000] # 1000 reviews for training\n",
    "train_sentiments = sentiments[:1000]\n",
    "test_reviews = reviews[1000:1300] # 300 reviews for testing\n",
    "test_sentiments = sentiments[1000:1300]\n",
    "\n",
    "# normalize datasets\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('but')\n",
    "stop_words.remove('not')\n",
    "\n",
    "norm_train_reviews = tn.normalize_corpus(train_reviews, stopwords=stop_words)\n",
    "norm_test_reviews = tn.normalize_corpus(test_reviews, stopwords=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_reviews)\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_reviews)\n",
    "tv_test_features = tv.transform(norm_test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)   # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8033\n",
      "Precision: 0.8045\n",
      "Recall: 0.8033\n",
      "F1 Score: 0.8028\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.79      0.85      0.82       155\n",
      "    negative       0.82      0.76      0.79       145\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.81      0.80      0.80       300\n",
      "weighted avg       0.80      0.80      0.80       300\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        131       24\n",
      "        negative         35      110\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on BOW features\n",
    "lr_bow_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8433\n",
      "Precision: 0.8439\n",
      "Recall: 0.8433\n",
      "F1 Score: 0.8434\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.86      0.83      0.85       155\n",
      "    negative       0.83      0.86      0.84       145\n",
      "\n",
      "    accuracy                           0.84       300\n",
      "   macro avg       0.84      0.84      0.84       300\n",
      "weighted avg       0.84      0.84      0.84       300\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        129       26\n",
      "        negative         21      124\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n",
    "                                               train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                               test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8333\n",
      "Precision: 0.8352\n",
      "Recall: 0.8333\n",
      "F1 Score: 0.8333\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.86      0.81      0.83       155\n",
      "    negative       0.81      0.86      0.83       145\n",
      "\n",
      "    accuracy                           0.83       300\n",
      "   macro avg       0.83      0.83      0.83       300\n",
      "weighted avg       0.84      0.83      0.83       300\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        125       30\n",
      "        negative         20      125\n"
     ]
    }
   ],
   "source": [
    "svm_bow_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                             train_features=cv_train_features, train_labels=train_sentiments,\n",
    "                                             test_features=cv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.8467\n",
      "Precision: 0.8468\n",
      "Recall: 0.8467\n",
      "F1 Score: 0.8467\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.86      0.85      0.85       155\n",
      "    negative       0.84      0.85      0.84       145\n",
      "\n",
      "    accuracy                           0.85       300\n",
      "   macro avg       0.85      0.85      0.85       300\n",
      "weighted avg       0.85      0.85      0.85       300\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        131       24\n",
      "        negative         22      123\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n",
    "                                                train_features=tv_train_features, train_labels=train_sentiments,\n",
    "                                                test_features=tv_test_features, test_labels=test_sentiments)\n",
    "meu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n",
    "                                      classes=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> TF-IDF representation is better than bag of words representation. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">  **3. Text similarity and clustering** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> Lexical similarity: This involves observing the contents of the text documents with regards to its syntax, structure, and content and measuring their similarity based on these parameters.\n",
    "*  Semantic similarity will not be introduced here.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">\n",
    "<span style=\"color:red\">Goal</span>: Building movie recommendations with document similarity\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4800 entries, 0 to 4802\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   title        4800 non-null   object \n",
      " 1   tagline      4800 non-null   object \n",
      " 2   overview     4800 non-null   object \n",
      " 3   genres       4800 non-null   object \n",
      " 4   popularity   4800 non-null   float64\n",
      " 5   description  4800 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 262.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tmdb_5000_movies.csv.gz', compression='gzip')  # load the dataset\n",
    "df = df[['title', 'tagline', 'overview', 'genres', 'popularity']]\n",
    "df.tagline.fillna('', inplace=True)\n",
    "df['description'] = df['tagline'].map(str) + ' ' + df['overview']\n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Cosine similarity** </font> <br>\n",
    "<center>  <font size=\"5\"> $\\cos(x,y)=\\frac{x. y^T}{|x|\\times|y|}$ </font> <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "\n",
    "norm_corpus = normalize_corpus(list(df['description']))\n",
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4800, 20667)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract TF-IDF features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tfidf_matrix = tf.fit_transform(norm_corpus)\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> **Pairwise document similarity** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_sim = cosine_similarity(tfidf_matrix)\n",
    "doc_sim_df = pd.DataFrame(doc_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4790</th>\n",
       "      <th>4791</th>\n",
       "      <th>4792</th>\n",
       "      <th>4793</th>\n",
       "      <th>4794</th>\n",
       "      <th>4795</th>\n",
       "      <th>4796</th>\n",
       "      <th>4797</th>\n",
       "      <th>4798</th>\n",
       "      <th>4799</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019030</td>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.024901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.033549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014564</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.034688</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.015854</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>0.023172</td>\n",
       "      <td>0.027452</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.027735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.028687</td>\n",
       "      <td>0.041623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022912</td>\n",
       "      <td>0.028676</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.017145</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.028354</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.016354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>0.013142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012920</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.015781</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030132</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006692</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 4800 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.010701  0.000000  0.019030  0.028687  0.024901  0.000000   \n",
       "1     0.010701  1.000000  0.011891  0.000000  0.041623  0.000000  0.014564   \n",
       "2     0.000000  0.011891  1.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3     0.019030  0.000000  0.000000  1.000000  0.008793  0.000000  0.015976   \n",
       "4     0.028687  0.041623  0.000000  0.008793  1.000000  0.000000  0.022912   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4795  0.000000  0.012593  0.000000  0.000000  0.010760  0.000000  0.024914   \n",
       "4796  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4797  0.006892  0.022391  0.011682  0.028354  0.010514  0.016354  0.000000   \n",
       "4798  0.000000  0.013724  0.000000  0.021785  0.000000  0.000000  0.016650   \n",
       "4799  0.000000  0.000000  0.004000  0.027735  0.000000  0.000000  0.030132   \n",
       "\n",
       "          7         8         9     ...      4790  4791      4792      4793  \\\n",
       "0     0.026516  0.000000  0.007420  ...  0.009702   0.0  0.023336  0.033549   \n",
       "1     0.027122  0.034688  0.007614  ...  0.009956   0.0  0.004818  0.000000   \n",
       "2     0.022242  0.015854  0.004891  ...  0.042617   0.0  0.000000  0.000000   \n",
       "3     0.023172  0.027452  0.073610  ...  0.000000   0.0  0.009667  0.000000   \n",
       "4     0.028676  0.000000  0.023538  ...  0.014800   0.0  0.000000  0.000000   \n",
       "...        ...       ...       ...  ...       ...   ...       ...       ...   \n",
       "4795  0.017145  0.020977  0.010038  ...  0.000000   0.0  0.013552  0.000000   \n",
       "4796  0.000000  0.000000  0.000000  ...  0.000000   0.0  0.000000  0.000000   \n",
       "4797  0.000000  0.023586  0.013142  ...  0.006412   0.0  0.012920  0.016919   \n",
       "4798  0.007292  0.015781  0.008183  ...  0.000000   0.0  0.000000  0.011651   \n",
       "4799  0.003443  0.000000  0.016476  ...  0.008981   0.0  0.018568  0.007730   \n",
       "\n",
       "          4794      4795  4796      4797      4798      4799  \n",
       "0     0.000000  0.000000   0.0  0.006892  0.000000  0.000000  \n",
       "1     0.000000  0.012593   0.0  0.022391  0.013724  0.000000  \n",
       "2     0.016519  0.000000   0.0  0.011682  0.000000  0.004000  \n",
       "3     0.000000  0.000000   0.0  0.028354  0.021785  0.027735  \n",
       "4     0.000000  0.010760   0.0  0.010514  0.000000  0.000000  \n",
       "...        ...       ...   ...       ...       ...       ...  \n",
       "4795  0.003977  1.000000   0.0  0.000000  0.004570  0.000000  \n",
       "4796  0.000000  0.000000   1.0  0.000000  0.000000  0.000000  \n",
       "4797  0.000000  0.000000   0.0  1.000000  0.007148  0.000000  \n",
       "4798  0.000000  0.004570   0.0  0.007148  1.000000  0.006692  \n",
       "4799  0.000000  0.000000   0.0  0.000000  0.006692  1.000000  \n",
       "\n",
       "[4800 rows x 4800 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sim_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> Let's find movies similar to ***Minions***.  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"file/minion.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "546"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find movie ID\n",
    "movies_list = df['title'].values\n",
    "minions_idx = np.where(movies_list == 'Minions')[0][0]\n",
    "minions_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0104544 , 0.01072835, 0.        , ..., 0.00690954, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get movie similarities\n",
    "movie_similarities = doc_sim_df.iloc[minions_idx].values\n",
    "movie_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([506, 614, 241, 813, 154])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 5 similar movie IDs\n",
    "similar_movie_idxs = np.argsort(-movie_similarities)[1:6] # the 1st is itself\n",
    "similar_movie_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Despicable Me 2', 'Despicable Me',\n",
       "       'Teenage Mutant Ninja Turtles: Out of the Shadows', 'Superman',\n",
       "       'Rise of the Guardians'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the names of top 5 similar movies\n",
    "similar_movies = movies_list[similar_movie_idxs]\n",
    "similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a movie recommender function\n",
    "def movie_recommender(movie_title, movies=movies_list, doc_sims=doc_sim_df):\n",
    "    # find movie id\n",
    "    movie_idx = np.where(movies == movie_title)[0][0]\n",
    "    # get movie similarities\n",
    "    movie_similarities = doc_sims.iloc[movie_idx].values\n",
    "    # get top 5 similar movie IDs\n",
    "    similar_movie_idxs = np.argsort(-movie_similarities)[1:6]\n",
    "    # get top 5 movies\n",
    "    similar_movies = movies[similar_movie_idxs]\n",
    "    # return the top 5 movies\n",
    "    return similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 popular movies in the list\n",
    "pop_movies = df.sort_values(by='popularity', ascending=False)\n",
    "popular_movies = pop_movies.head(10)['title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie: Minions\n",
      "Top 5 recommended Movies: ['Despicable Me 2' 'Despicable Me'\n",
      " 'Teenage Mutant Ninja Turtles: Out of the Shadows' 'Superman'\n",
      " 'Rise of the Guardians']\n",
      "\n",
      "Movie: Interstellar\n",
      "Top 5 recommended Movies: ['Gattaca' 'Space Pirate Captain Harlock' 'Space Cowboys'\n",
      " 'Starship Troopers' 'Final Destination 2']\n",
      "\n",
      "Movie: Deadpool\n",
      "Top 5 recommended Movies: ['Silent Trigger' 'Underworld: Evolution' 'Bronson' 'Shaft' 'Don Jon']\n",
      "\n",
      "Movie: Guardians of the Galaxy\n",
      "Top 5 recommended Movies: ['Chasing Mavericks' 'E.T. the Extra-Terrestrial' 'American Sniper'\n",
      " 'The Amazing Spider-Man 2' 'Hoop Dreams']\n",
      "\n",
      "Movie: Mad Max: Fury Road\n",
      "Top 5 recommended Movies: ['The 6th Day' 'Star Trek Beyond' 'Kites' 'The Orphanage'\n",
      " 'The Water Diviner']\n",
      "\n",
      "Movie: Jurassic World\n",
      "Top 5 recommended Movies: ['Jurassic Park' 'The Lost World: Jurassic Park' 'The Nut Job'\n",
      " \"National Lampoon's Vacation\" 'Vacation']\n",
      "\n",
      "Movie: Pirates of the Caribbean: The Curse of the Black Pearl\n",
      "Top 5 recommended Movies: [\"Pirates of the Caribbean: Dead Man's Chest\" 'The Pirate'\n",
      " 'Pirates of the Caribbean: On Stranger Tides'\n",
      " 'The Pirates! In an Adventure with Scientists!' 'Joyful Noise']\n",
      "\n",
      "Movie: Dawn of the Planet of the Apes\n",
      "Top 5 recommended Movies: ['Battle for the Planet of the Apes' 'Groove' 'The Other End of the Line'\n",
      " 'Chicago Overcoat' 'Definitely, Maybe']\n",
      "\n",
      "Movie: The Hunger Games: Mockingjay - Part 1\n",
      "Top 5 recommended Movies: ['The Hunger Games: Catching Fire' 'The Hunger Games: Mockingjay - Part 2'\n",
      " 'John Carter' 'For Greater Glory - The True Story of Cristiada'\n",
      " 'The Proposition']\n",
      "\n",
      "Movie: Big Hero 6\n",
      "Top 5 recommended Movies: ['Wreck-It Ralph' 'A Home at the End of the World' 'Phat Girlz' 'Splice'\n",
      " 'U.F.O.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find 5 top similar movies for each\n",
    "for movie in popular_movies:\n",
    "    print('Movie:', movie)\n",
    "    print('Top 5 recommended Movies:', movie_recommender(movie_title=movie))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\"> Other ways to determine the similarity are not discussed here. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"5\"> **Clustering** </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=\"4\"> **Steps of the k-means algorithm**</font> \n",
    "<img src=\"file/k_means.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=10, n_clusters=6, random_state=42)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 6\n",
    "km = KMeans(n_clusters=NUM_CLUSTERS, max_iter=10, n_init=10, random_state=42).fit(tfidf_matrix)\n",
    "km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kmeans_cluster'] = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_clusters = (df[['title', 'kmeans_cluster', 'popularity']]\n",
    "                  .sort_values(by=['kmeans_cluster', 'popularity'], \n",
    "                               ascending=False)\n",
    "                  .groupby('kmeans_cluster').head(20))\n",
    "movie_clusters = movie_clusters.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER #1\n",
      "Key Features: ['war', 'world', 'story', 'true story', 'true', 'world war', 'based', 'war ii', 'ii', 'us']\n",
      "Popular Movies: ['Captain America: Civil War', 'The Dark Knight', 'Batman v Superman: Dawn of Justice', 'Avatar', 'The Imitation Game', 'Fury', 'X-Men: Apocalypse', 'The Hunger Games: Mockingjay - Part 2', 'X-Men: Days of Future Past', 'Transformers: Age of Extinction', 'Chappie', 'Quantum of Solace', 'Spectre', \"Schindler's List\", 'The Matrix', 'Man of Steel', 'Skyfall', 'The Adventures of Tintin', 'Casino Royale', 'The Good, the Bad and the Ugly']\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #2\n",
      "Key Features: ['get', 'back', 'find', 'one', 'man', 'young', 'must', 'hes', 'town', 'time']\n",
      "Popular Movies: ['Minions', 'Interstellar', 'Guardians of the Galaxy', 'Jurassic World', 'Pirates of the Caribbean: The Curse of the Black Pearl', 'The Hunger Games: Mockingjay - Part 1', 'Whiplash', 'The Martian', 'Fight Club', \"Pirates of the Caribbean: Dead Man's Chest\", 'The Avengers', 'Gone Girl', 'Rise of the Planet of the Apes', 'The Lord of the Rings: The Fellowship of the Ring', 'Pirates of the Caribbean: On Stranger Tides', 'Avengers: Age of Ultron', 'Harry Potter and the Chamber of Secrets', \"One Flew Over the Cuckoo's Nest\", 'Star Wars', 'The Hobbit: The Battle of the Five Armies']\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #3\n",
      "Key Features: ['film', 'one', 'two', 'crime', 'movie', 'story', 'together', 'team', 'time', 'angeles']\n",
      "Popular Movies: ['Dawn of the Planet of the Apes', 'Big Hero 6', 'Terminator Genisys', 'Inception', 'The Maze Runner', 'Tomorrowland', 'Brave', 'The Lord of the Rings: The Return of the King', 'Pulp Fiction', 'Spider-Man 3', 'Night at the Museum: Secret of the Tomb', 'Mission: Impossible - Rogue Nation', 'Despicable Me', 'Maze Runner: The Scorch Trials', 'Men in Black', 'Insurgent', \"We're the Millers\", 'The Jungle Book', 'Alien', 'Blade Runner']\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #4\n",
      "Key Features: ['life', 'family', 'love', 'story', 'man', 'young', 'one', 'two', 'father', 'woman']\n",
      "Popular Movies: ['Deadpool', 'Mad Max: Fury Road', 'Frozen', 'The Godfather', \"Pirates of the Caribbean: At World's End\", 'Forrest Gump', 'The Shawshank Redemption', 'Inside Out', 'Twilight', 'Maleficent', 'Bruce Almighty', 'The Hobbit: An Unexpected Journey', 'The Twilight Saga: Eclipse', 'Furious 7', 'Titanic', 'The Twilight Saga: Breaking Dawn - Part 2', 'Fifty Shades of Grey', 'Gladiator', 'Psycho', 'Up']\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #5\n",
      "Key Features: ['new', 'york', 'new york', 'city', 'york city', 'one', 'young', 'two', 'love', 'world']\n",
      "Popular Movies: ['Teenage Mutant Ninja Turtles', 'Pixels', 'Despicable Me 2', 'Batman Begins', 'The Dark Knight Rises', 'The Godfather: Part II', 'How to Train Your Dragon 2', '12 Years a Slave', 'The Wolf of Wall Street', 'The Bourne Legacy', 'The Devil Wears Prada', 'Non-Stop', 'Horrible Bosses 2', 'Sherlock Holmes: A Game of Shadows', 'Home Alone 2: Lost in New York', 'Captain America: The Winter Soldier', 'I Am Legend', 'Dumb and Dumber To', 'Star Trek Beyond', 'District 9']\n",
      "--------------------------------------------------------------------------------\n",
      "CLUSTER #6\n",
      "Key Features: ['school', 'high', 'high school', 'girl', 'teacher', 'one', 'friends', 'young', 'students', 'love']\n",
      "Popular Movies: ['The Fifth Element', 'It Follows', 'The Amazing Spider-Man 2', 'Monsters University', 'Spider-Man', 'Grease', 'Paper Towns', 'Chronicle', 'Nerve', '21 Jump Street', '50 First Dates', 'Top Gun', 'Lilo & Stitch', 'Dead Poets Society', '22 Jump Street', 'Project X', 'Live Free or Die Hard', 'Step Up', 'Easy A', '21']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "topn_features = 10\n",
    "ordered_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# get key features for each cluster\n",
    "# get movies belonging to each cluster\n",
    "for cluster_num in range(NUM_CLUSTERS):\n",
    "    key_features = [feature_names[index] \n",
    "                        for index in ordered_centroids[cluster_num, :topn_features]]\n",
    "    movies = movie_clusters[movie_clusters['kmeans_cluster'] == cluster_num]['title'].values.tolist()\n",
    "    print('CLUSTER #'+str(cluster_num+1))\n",
    "    print('Key Features:', key_features)\n",
    "    print('Popular Movies:', movies)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **Some other topics in the natural language processing** </font> \n",
    "* <font size=\"4\"> <span style=\"color:blue\"> Text summarization and topic models </span></font> <br>\n",
    "<font size=\"3\"> including key-phrase extraction, topic modeling and document summarization </font> \n",
    "* <font size=\"4\"> <span style=\"color:blue\"> Semantic analysis </span> </font> <br>\n",
    "<font size=\"3\"> Semantic analysis is more about understanding the actual context and meaning behind words in text and how they relate to other words and convey some information as a whole.\n",
    "</font>\n",
    "* <font size=\"4\"> <span style=\"color:blue\"> Deep learning $\\rightarrow$ word/sentence-embedding models </span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **Summary** </font>\n",
    "* <font size=\"4\"> A brief introduction to the natural language processing, giving you a holistic feeling that when we talk about natural language processing what we are actually discussing.</font> <br>\n",
    "* <font size=\"4\"> Not an introduction to machine learning algorithms.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Reference: </font> <br>\n",
    "<font size=\"4\"> \n",
    "    **Text Analytics with Python, A Practitioner’s Guide to NaturalLanguage Processing**  by *Dipanjan Sarkar*\n",
    "    **Introduction to Machine Learning with Python, A Guide for Data Scientists** by *Andreas C. Müller* and *Sarah Guido*\n",
    "    **Natural Language Processing Recipes, Unlocking Text Data with Machine Learning and Deep Learning using Python** by *Akshay Kulkarni* and *Adarsha Shivananda*\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "<font size=\"10\"> <center>Thanks for your attention!</center> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
